{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사운드바 모델 매핑 Agent (Databricks용)\n",
    "\n",
    "로그 CSV의 관측 기기명(NAME1~4, NAME_BT)을 표준 canonical 사운드바 모델명으로 매핑합니다.\n",
    "**모든 로직이 이 노트북 안에 포함되어 있어, Databricks에 노트북만 올려서 실행할 수 있습니다.**\n",
    "\n",
    "1. 경로 설정\n",
    "2. 공통 모듈 코드 (정규화, DB, 검색, 검증, Agent)\n",
    "3. 데이터 로드 및 예측\n",
    "4. 결과 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 경로 설정\n",
    "\n",
    "Databricks에서는 DBFS 경로(예: `/dbfs/FileStore/...`) 또는 위젯으로 경로를 지정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 아래 경로를 환경에 맞게 수정하세요. Databricks: /dbfs/FileStore/... 등\n",
    "INPUT_CSV = Path(\"/path/to/HDMI_BT_Log.csv\")\n",
    "SOUNDBAR_DB = Path(\"/path/to/soundbar_list.py\")\n",
    "OUTPUT_PRED = Path(\"/path/to/output/pred.csv\")\n",
    "\n",
    "OUTPUT_PRED.parent.mkdir(parents=True, exist_ok=True)\n",
    "print(\"INPUT_CSV:\", INPUT_CSV, \"존재:\", INPUT_CSV.exists())\n",
    "print(\"SOUNDBAR_DB:\", SOUNDBAR_DB, \"존재:\", SOUNDBAR_DB.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 공통 모듈 코드 (정규화, 사운드바 DB, 로그 질의, 브랜드 추출, 비사운드바 탐지, 임베딩 검색, 검증, Agent)\n",
    "\n",
    "아래 셀들을 순서대로 실행하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화 (normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "텍스트 정규화 유틸리티 모듈입니다.\n",
    "\n",
    "본 프로젝트는 로그 문자열과 DB 모델 문자열 간의 매칭을 수행하므로,\n",
    "대소문자/특수문자/공백 등 표면적인 차이를 최소화하는 정규화가 중요합니다.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "\n",
    "_WS_RE = re.compile(r\"\\s+\")\n",
    "_BRACKET_RE = re.compile(r\"\\([^)]*\\)\")\n",
    "_KEEP_CHARS_RE = re.compile(r\"[^A-Z0-9\\.\\+/\\- ]+\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NormalizationConfig:\n",
    "    \"\"\"정규화 동작을 제어하는 설정값입니다.\"\"\"\n",
    "\n",
    "    drop_bracketed: bool = True\n",
    "    keep_dot_plus_slash_dash: bool = True\n",
    "    uppercase: bool = True\n",
    "    collapse_whitespace: bool = True\n",
    "    remove_hyphen: bool = True  # HT-S40R -> HTS40R 매칭 개선\n",
    "\n",
    "\n",
    "def normalize_text(text: Optional[str], config: Optional[NormalizationConfig] = None) -> str:\n",
    "    \"\"\"\n",
    "    주어진 문자열을 매칭 친화적으로 정규화합니다.\n",
    "\n",
    "    - 대문자 변환\n",
    "    - 괄호(...) 제거(옵션)\n",
    "    - 영숫자/일부 기호(. + / -) 이외 제거\n",
    "    - 공백 정리\n",
    "\n",
    "    Args:\n",
    "        text: 입력 텍스트 (None 가능).\n",
    "        config: 정규화 설정. None이면 기본값 사용.\n",
    "\n",
    "    Returns:\n",
    "        정규화된 문자열(빈 문자열 가능).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    cfg = config or NormalizationConfig()\n",
    "\n",
    "    s = text.strip()\n",
    "    if cfg.uppercase:\n",
    "        s = s.upper()\n",
    "    if cfg.drop_bracketed:\n",
    "        # 예: \"LG SQC2(CC)\" -> \"LG SQC2\"\n",
    "        s = _BRACKET_RE.sub(\" \", s)\n",
    "\n",
    "    if cfg.keep_dot_plus_slash_dash:\n",
    "        s = _KEEP_CHARS_RE.sub(\" \", s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^A-Z0-9 ]+\", \" \", s)\n",
    "\n",
    "    if cfg.collapse_whitespace:\n",
    "        s = _WS_RE.sub(\" \", s).strip()\n",
    "    if cfg.remove_hyphen:\n",
    "        s = s.replace(\"-\", \"\").strip()\n",
    "        if cfg.collapse_whitespace:\n",
    "            s = _WS_RE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_brand(brand: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    브랜드 문자열을 정규화합니다.\n",
    "\n",
    "    Args:\n",
    "        brand: 브랜드(제조사) 문자열.\n",
    "\n",
    "    Returns:\n",
    "        정규화된 브랜드 문자열(대문자, 공백 정리).\n",
    "    \"\"\"\n",
    "    return normalize_text(brand)\n",
    "\n",
    "\n",
    "def unique_preserve_order(items: Iterable[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    입력 시퀀스에서 중복을 제거하되, 최초 등장 순서를 유지합니다.\n",
    "\n",
    "    Args:\n",
    "        items: 문자열 iterable.\n",
    "\n",
    "    Returns:\n",
    "        중복 제거된 리스트.\n",
    "    \"\"\"\n",
    "    seen: set[str] = set()\n",
    "    out: list[str] = []\n",
    "    for x in items:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사운드바 DB (soundbar_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "사운드바 표준 모델 DB 로드/파싱/정규화 모듈입니다.\n",
    "\n",
    "현재 워크스페이스의 `soundbar_list.py`는 `data = [...]` 형태로 모델 목록을 보유합니다.\n",
    "각 항목은 아래 구조(공백 구분 토큰)로 가정합니다.\n",
    "\n",
    "- 첫 토큰: Brand\n",
    "- 마지막 3토큰: Grade, Year, supportDolbyAtmos (혹은 'null')\n",
    "- 중간 토큰: Model (공백이 있을 수 있으므로 join)\n",
    "\n",
    "파일 입출력은 오류 처리를 포함합니다.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SoundbarRecord:\n",
    "    \"\"\"사운드바 DB의 단일 레코드입니다.\"\"\"\n",
    "\n",
    "    brand: str\n",
    "    model: str\n",
    "    grade: Optional[str]\n",
    "    year: Optional[int]\n",
    "    support_dolby_atmos: Optional[bool]\n",
    "    canonical: str\n",
    "\n",
    "\n",
    "def _parse_optional_int(token: str) -> Optional[int]:\n",
    "    \"\"\"문자열 토큰을 int로 변환합니다. 실패 시 None을 반환합니다.\"\"\"\n",
    "    token = token.strip()\n",
    "    if not token or token.lower() == \"null\":\n",
    "        return None\n",
    "    try:\n",
    "        return int(token)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _parse_optional_bool(token: str) -> Optional[bool]:\n",
    "    \"\"\"문자열 토큰을 bool로 변환합니다. 실패 시 None을 반환합니다.\"\"\"\n",
    "    token = token.strip()\n",
    "    if not token or token.lower() == \"null\":\n",
    "        return None\n",
    "    if token.upper() in {\"O\", \"Y\", \"YES\", \"TRUE\", \"T\"}:\n",
    "        return True\n",
    "    if token.upper() in {\"X\", \"N\", \"NO\", \"FALSE\", \"F\"}:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_soundbar_item(item: str) -> Optional[SoundbarRecord]:\n",
    "    \"\"\"\n",
    "    `soundbar_list.py`의 data 항목(문자열 1개)을 파싱합니다.\n",
    "\n",
    "    Args:\n",
    "        item: 예) \"LG S90TY High 2024 O\"\n",
    "\n",
    "    Returns:\n",
    "        SoundbarRecord 또는 파싱 실패 시 None.\n",
    "    \"\"\"\n",
    "    if not item or not item.strip():\n",
    "        return None\n",
    "    tokens = item.split()\n",
    "    if len(tokens) < 2:\n",
    "        return None\n",
    "\n",
    "    brand_raw = tokens[0]\n",
    "    brand = normalize_brand(brand_raw)\n",
    "\n",
    "    grade: Optional[str] = None\n",
    "    year: Optional[int] = None\n",
    "    atmos: Optional[bool] = None\n",
    "    model_tokens: list[str] = tokens[1:]\n",
    "\n",
    "    # 최소 5토큰 이상이면 마지막 3토큰을 메타로 간주\n",
    "    if len(tokens) >= 5:\n",
    "        grade_token = tokens[-3]\n",
    "        year_token = tokens[-2]\n",
    "        atmos_token = tokens[-1]\n",
    "        grade = None if grade_token.lower() == \"null\" else normalize_text(grade_token)\n",
    "        year = _parse_optional_int(year_token)\n",
    "        atmos = _parse_optional_bool(atmos_token)\n",
    "        model_tokens = tokens[1:-3]\n",
    "\n",
    "    model = normalize_text(\" \".join(model_tokens))\n",
    "    if not brand or not model:\n",
    "        return None\n",
    "    canonical = f\"{brand} {model}\".strip()\n",
    "\n",
    "    return SoundbarRecord(\n",
    "        brand=brand,\n",
    "        model=model,\n",
    "        grade=grade,\n",
    "        year=year,\n",
    "        support_dolby_atmos=atmos,\n",
    "        canonical=canonical,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_soundbar_db_from_py(soundbar_list_py: Path) -> list[SoundbarRecord]:\n",
    "    \"\"\"\n",
    "    `soundbar_list.py`에서 soundbar DB를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        soundbar_list_py: `soundbar_list.py` 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        SoundbarRecord 리스트(중복 canonical은 최초 1개 유지).\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: 파일이 없을 때.\n",
    "        OSError: 파일 읽기 실패.\n",
    "        ValueError: `data = [...]` 파싱 실패.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = soundbar_list_py.read_text(encoding=\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        raise\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"사운드바 DB 파일 읽기 실패: {soundbar_list_py}\") from e\n",
    "\n",
    "    # soundbar_list.py에서 `data = [...]`를 안전하게 추출하기 위해 ast 사용\n",
    "    try:\n",
    "        module = ast.parse(text)\n",
    "    except SyntaxError as e:\n",
    "        raise ValueError(f\"soundbar_list.py 파싱 실패: {soundbar_list_py}\") from e\n",
    "\n",
    "    data_value = None\n",
    "    for node in module.body:\n",
    "        if isinstance(node, ast.Assign):\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name) and target.id == \"data\":\n",
    "                    data_value = node.value\n",
    "                    break\n",
    "        if data_value is not None:\n",
    "            break\n",
    "\n",
    "    if data_value is None:\n",
    "        raise ValueError(\"soundbar_list.py에서 `data` 변수를 찾지 못했습니다.\")\n",
    "\n",
    "    try:\n",
    "        data_list = ast.literal_eval(data_value)\n",
    "    except Exception as e:  # noqa: BLE001 - 안전한 오류 래핑\n",
    "        raise ValueError(\"`data = [...]` 값을 literal_eval로 해석하지 못했습니다.\") from e\n",
    "\n",
    "    if not isinstance(data_list, list):\n",
    "        raise ValueError(\"`data`는 list 타입이어야 합니다.\")\n",
    "\n",
    "    records: list[SoundbarRecord] = []\n",
    "    canonicals: list[str] = []\n",
    "    tmp: dict[str, SoundbarRecord] = {}\n",
    "    for item in data_list:\n",
    "        if not isinstance(item, str):\n",
    "            continue\n",
    "        rec = parse_soundbar_item(item)\n",
    "        if rec is None:\n",
    "            continue\n",
    "        canonicals.append(rec.canonical)\n",
    "        # 중복 canonical은 최초 등장 유지\n",
    "        if rec.canonical not in tmp:\n",
    "            tmp[rec.canonical] = rec\n",
    "\n",
    "    ordered = unique_preserve_order(canonicals)\n",
    "    for c in ordered:\n",
    "        r = tmp.get(c)\n",
    "        if r:\n",
    "            records.append(r)\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_brand_set(records: Iterable[SoundbarRecord]) -> set[str]:\n",
    "    \"\"\"\n",
    "    레코드 리스트로부터 브랜드 집합을 구합니다.\n",
    "\n",
    "    Args:\n",
    "        records: SoundbarRecord iterable.\n",
    "\n",
    "    Returns:\n",
    "        브랜드 집합(정규화된 문자열).\n",
    "    \"\"\"\n",
    "    return {r.brand for r in records if r.brand}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로그 질의 (log_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "로그 행에서 사운드바 후보 문자열(query)을 구성하고, placeholder/노이즈를 필터링합니다.\n",
    "\n",
    "`raw_data/HDMI_BT_Log.csv`는 여러 입력 소스(NAME1~4)와 브랜드(BRAND1~4),\n",
    "그리고 BT 장치명(NAME_BT)을 포함합니다. 여기서 실제 사운드바 모델 매칭에\n",
    "의미있는 후보 문자열을 추출하는 것이 목적입니다.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable, Optional\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "_HDMI_RE = re.compile(r\"^HDMI(\\s*\\d+)?$\")\n",
    "_PLACEHOLDER_EXACT = {\n",
    "    \"AV\",\n",
    "    \"AUX\",\n",
    "    \"USB\",\n",
    "    \"OPTICAL\",\n",
    "    \"DIGITAL\",\n",
    "    \"DIGITAL IN\",\n",
    "    \"TV\",\n",
    "    \"TV IN\",\n",
    "    \"IN\",\n",
    "    \"HDMI\",\n",
    "}\n",
    "\n",
    "_BT_HEADPHONE_HINT_RE = re.compile(\n",
    "    r\"\\b(AIRPODS|EARBUD|EARBUDS|HEADPHONE|HEADPHONES|TUNE|TOUR|LIVE|WH-|WF-)\\b\"\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LogQuery:\n",
    "    \"\"\"\n",
    "    로그에서 추출한 후보 검색 질의 문자열입니다.\n",
    "\n",
    "    Attributes:\n",
    "        query: 정규화된 검색 질의\n",
    "        raw: 원본 문자열(가능하면 유지)\n",
    "        source: 어떤 필드에서 왔는지(예: NAME2, NAME_BT 등)\n",
    "        weight: 이후 검색/검증에서 가중치로 사용할 값(기본 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    query: str\n",
    "    raw: str\n",
    "    source: str\n",
    "    weight: float = 1.0\n",
    "\n",
    "\n",
    "def is_placeholder(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    HDMI/AV 같은 placeholder 입력 소스명을 탐지합니다.\n",
    "\n",
    "    주의: \\\"APPLE TV\\\" 같은 정상 장치명은 placeholder로 처리하지 않도록\n",
    "    짧은 토큰/정규식 기반으로만 필터합니다.\n",
    "\n",
    "    Args:\n",
    "        text: 정규화된 문자열(권장: `normalize_text` 결과)\n",
    "\n",
    "    Returns:\n",
    "        placeholder로 판단되면 True\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return True\n",
    "    s = text.strip().upper()\n",
    "    if _HDMI_RE.match(s):\n",
    "        return True\n",
    "    if s in _PLACEHOLDER_EXACT:\n",
    "        return True\n",
    "    # 매우 짧고 의미없는 입력은 제외\n",
    "    if len(s) <= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def bt_looks_like_non_soundbar(bt_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    BT 장치명이 헤드폰/이어폰 등 사운드바가 아닐 가능성이 높은지 휴리스틱으로 판단합니다.\n",
    "\n",
    "    Args:\n",
    "        bt_name: 정규화된 BT 이름\n",
    "\n",
    "    Returns:\n",
    "        사운드바가 아닐 가능성이 높으면 True\n",
    "    \"\"\"\n",
    "    if not bt_name:\n",
    "        return False\n",
    "    s = bt_name.upper()\n",
    "    return _BT_HEADPHONE_HINT_RE.search(s) is not None\n",
    "\n",
    "\n",
    "def build_log_queries_from_row(\n",
    "    row: dict[str, Any],\n",
    "    soundbar_brand_set: Optional[set[str]] = None,\n",
    "    include_bt: bool = True,\n",
    ") -> list[LogQuery]:\n",
    "    \"\"\"\n",
    "    단일 로그 row에서 후보 질의 문자열 리스트를 생성합니다.\n",
    "\n",
    "    생성 규칙(요약):\n",
    "    - NAME1~4에서 placeholder 제외 후 후보 생성\n",
    "    - BRANDi가 있으면 \\\"BRANDi NAMEi\\\" 형태 후보를 추가(브랜드 강화)\n",
    "    - NAME_BT는 노이즈(헤드폰/이어폰) 가능성이 높아 기본 weight를 낮춰 추가(옵션)\n",
    "\n",
    "    Args:\n",
    "        row: pandas row를 `to_dict()`한 형태를 권장.\n",
    "        soundbar_brand_set: 사운드바 DB 기반 브랜드 집합(정규화된 값).\n",
    "            제공되면 브랜드가 DB에 있는 경우 가중치/우선순위를 강화합니다.\n",
    "        include_bt: NAME_BT를 후보로 포함할지 여부.\n",
    "\n",
    "    Returns:\n",
    "        LogQuery 리스트(중복 제거, 순서 유지).\n",
    "    \"\"\"\n",
    "    candidates: list[LogQuery] = []\n",
    "\n",
    "    def _is_missing(x: Any) -> bool:\n",
    "        \"\"\"\n",
    "        pandas NaN 등 결측값을 탐지합니다.\n",
    "\n",
    "        주의:\n",
    "        - CSV/전처리 과정에 따라 결측이 float NaN이 아니라 문자열 \"nan\"/\"NaN\"으로 들어올 수 있습니다.\n",
    "          이 경우 query가 \"NAN CHROMECAST\"처럼 오염되어 매칭 오탐을 유발할 수 있어,\n",
    "          문자열 결측도 결측으로 처리합니다.\n",
    "        \"\"\"\n",
    "        if x is None:\n",
    "            return True\n",
    "        if isinstance(x, float) and math.isnan(x):\n",
    "            return True\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip().lower()\n",
    "            if s in {\"nan\", \"none\", \"null\", \"\"}:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _add(query_raw: str, source: str, weight: float) -> None:\n",
    "        q = normalize_text(query_raw)\n",
    "        if not q or is_placeholder(q):\n",
    "            return\n",
    "        candidates.append(LogQuery(query=q, raw=str(query_raw), source=source, weight=weight))\n",
    "\n",
    "    # NAME1~4 / BRAND1~4\n",
    "    for i in range(1, 5):\n",
    "        name_key = f\"NAME{i}\"\n",
    "        brand_key = f\"BRAND{i}\"\n",
    "        name_raw = row.get(name_key, \"\")\n",
    "        brand_raw = row.get(brand_key, \"\")\n",
    "\n",
    "        name = \"\" if _is_missing(name_raw) else normalize_text(str(name_raw))\n",
    "        brand = \"\" if _is_missing(brand_raw) else normalize_brand(str(brand_raw))\n",
    "\n",
    "        if name and not is_placeholder(name):\n",
    "            _add(str(name_raw), name_key, weight=1.0)\n",
    "\n",
    "            if brand:\n",
    "                w = 1.1\n",
    "                if soundbar_brand_set and brand in soundbar_brand_set:\n",
    "                    w = 1.25\n",
    "                # 모델명에 이미 브랜드가 있으면 모델명만, 없으면 브랜드+모델명\n",
    "                if name.upper().startswith(brand.upper()):\n",
    "                    brand_name_raw = str(name_raw).strip()\n",
    "                else:\n",
    "                    brand_name_raw = f\"{str(brand_raw).strip()} {str(name_raw).strip()}\".strip()\n",
    "                _add(brand_name_raw, f\"{brand_key}+{name_key}\", weight=w)\n",
    "\n",
    "    # NAME_BT (옵션)\n",
    "    if include_bt:\n",
    "        bt_raw = row.get(\"NAME_BT\", \"\")\n",
    "        bt = \"\" if _is_missing(bt_raw) else normalize_text(str(bt_raw))\n",
    "        if bt and not is_placeholder(bt):\n",
    "            weight = 0.63  # 0.6 → 0.63 (5% 상향, below_threshold 완화)\n",
    "            if bt_looks_like_non_soundbar(bt):\n",
    "                weight = 0.35\n",
    "            _add(str(bt_raw), \"NAME_BT\", weight=weight)\n",
    "\n",
    "    # 중복 제거(정규화 query 기준)\n",
    "    ordered_queries = unique_preserve_order([c.query for c in candidates])\n",
    "    first_by_query: dict[str, LogQuery] = {}\n",
    "    for c in candidates:\n",
    "        if c.query not in first_by_query:\n",
    "            first_by_query[c.query] = c\n",
    "\n",
    "    return [first_by_query[q] for q in ordered_queries if q in first_by_query]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TypedQuery:\n",
    "    \"\"\"\n",
    "    소스별(타입별) 검색 질의입니다. BT/HDMI 각각 독립 예측을 위해 사용합니다.\n",
    "\n",
    "    Attributes:\n",
    "        type_: \"BT\" 또는 \"HDMI\"\n",
    "        source: 필드명(NAME_BT, NAME1, NAME2, NAME3, NAME4)\n",
    "        query: 정규화된 검색 질의\n",
    "        raw: 원본 문자열(primary_query 출력용)\n",
    "        weight: 가중치\n",
    "    \"\"\"\n",
    "\n",
    "    type_: str\n",
    "    source: str\n",
    "    query: str\n",
    "    raw: str\n",
    "    weight: float = 1.0\n",
    "\n",
    "\n",
    "def build_typed_queries_from_row(\n",
    "    row: dict[str, Any],\n",
    "    soundbar_brand_set: Optional[set[str]] = None,\n",
    "    include_bt: bool = True,\n",
    ") -> list[TypedQuery]:\n",
    "    \"\"\"\n",
    "    단일 로그 row에서 소스별(BT/HDMI) 질의 리스트를 생성합니다.\n",
    "\n",
    "    - BT: NAME_BT가 유효하면 1개\n",
    "    - HDMI: (NAME1,BRAND1)~(NAME4,BRAND4) 각 쌍이 유효하면 1개씩, 최대 4개\n",
    "\n",
    "    각 소스별로 독립 예측을 수행할 때 사용합니다.\n",
    "\n",
    "    Args:\n",
    "        row: pandas row를 dict로 변환한 값\n",
    "        soundbar_brand_set: 사운드바 DB 브랜드 집합\n",
    "        include_bt: NAME_BT 포함 여부\n",
    "\n",
    "    Returns:\n",
    "        TypedQuery 리스트(소스별 1개, 중복 없음)\n",
    "    \"\"\"\n",
    "    result: list[TypedQuery] = []\n",
    "\n",
    "    def _is_missing(x: Any) -> bool:\n",
    "        if x is None:\n",
    "            return True\n",
    "        if isinstance(x, float) and math.isnan(x):\n",
    "            return True\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip().lower()\n",
    "            if s in {\"nan\", \"none\", \"null\", \"\"}:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # BT: NAME_BT\n",
    "    if include_bt:\n",
    "        bt_raw = row.get(\"NAME_BT\", \"\")\n",
    "        if not _is_missing(bt_raw):\n",
    "            bt = normalize_text(str(bt_raw))\n",
    "            if bt and not is_placeholder(bt):\n",
    "                w = 0.6 if bt_looks_like_non_soundbar(bt) else 0.6\n",
    "                result.append(\n",
    "                    TypedQuery(type_=\"BT\", source=\"NAME_BT\", query=bt, raw=str(bt_raw).strip(), weight=w)\n",
    "                )\n",
    "\n",
    "    # HDMI: (NAME1,BRAND1) ~ (NAME4,BRAND4)\n",
    "    for i in range(1, 5):\n",
    "        name_key = f\"NAME{i}\"\n",
    "        brand_key = f\"BRAND{i}\"\n",
    "        name_raw = row.get(name_key, \"\")\n",
    "        brand_raw = row.get(brand_key, \"\")\n",
    "\n",
    "        name = \"\" if _is_missing(name_raw) else normalize_text(str(name_raw))\n",
    "        brand = \"\" if _is_missing(brand_raw) else normalize_brand(str(brand_raw))\n",
    "\n",
    "        if not name or is_placeholder(name):\n",
    "            continue\n",
    "\n",
    "        # primary_query 출력: 모델명에 브랜드가 있으면 모델명만, 없으면 브랜드+모델명\n",
    "        if brand:\n",
    "            raw = (\n",
    "                str(name_raw).strip()\n",
    "                if name.upper().startswith(brand.upper())\n",
    "                else f\"{str(brand_raw).strip()} {str(name_raw).strip()}\".strip()\n",
    "            )\n",
    "        else:\n",
    "            raw = str(name_raw).strip()\n",
    "        if brand and not (name.upper().startswith(brand.upper())):\n",
    "            query = normalize_text(f\"{str(brand_raw).strip()} {str(name_raw).strip()}\")\n",
    "            w = 1.1\n",
    "            if soundbar_brand_set and brand in soundbar_brand_set:\n",
    "                w = 1.25\n",
    "        else:\n",
    "            query = name\n",
    "            w = 1.0\n",
    "\n",
    "        result.append(TypedQuery(type_=\"HDMI\", source=name_key, query=query, raw=raw, weight=w))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def choose_primary_query(queries: Iterable[LogQuery]) -> Optional[LogQuery]:\n",
    "    \"\"\"\n",
    "    여러 후보 LogQuery 중 1개를 대표 질의로 선택합니다.\n",
    "\n",
    "    현재는 가장 weight가 크고, 길이가 적당히 긴 후보를 우선합니다.\n",
    "\n",
    "    Args:\n",
    "        queries: LogQuery iterable\n",
    "\n",
    "    Returns:\n",
    "        선택된 LogQuery 또는 후보가 없으면 None\n",
    "    \"\"\"\n",
    "    qs = list(queries)\n",
    "    if not qs:\n",
    "        return None\n",
    "    return sorted(qs, key=lambda q: (q.weight, len(q.query)), reverse=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 브랜드 추출 (brand_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "기기명에서 브랜드(Brand)를 추출하는 모듈입니다.\n",
    "\n",
    "사운드바 DB의 known brand 집합을 사용하여 dictionary 기반 추출을 수행합니다.\n",
    "(일반 NER 대신 도메인 특화 방식으로 정확도를 높입니다.)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Set\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BrandExtraction:\n",
    "    \"\"\"\n",
    "    브랜드 추출 결과입니다.\n",
    "\n",
    "    Attributes:\n",
    "        brand: 추출된 브랜드(정규화된 문자열). 없으면 None.\n",
    "        model_part: 브랜드 제외 나머지 부분(정규화). 없으면 query 전체.\n",
    "        original_query: 원본 질의.\n",
    "    \"\"\"\n",
    "\n",
    "    brand: Optional[str]\n",
    "    model_part: str\n",
    "    original_query: str\n",
    "\n",
    "\n",
    "def extract_brand_from_query(\n",
    "    query: str,\n",
    "    known_brands: Set[str],\n",
    ") -> BrandExtraction:\n",
    "    \"\"\"\n",
    "    질의 문자열에서 브랜드를 추출합니다.\n",
    "\n",
    "    전략:\n",
    "    1. 정규화된 query의 첫 토큰이 known_brands에 있으면 brand로 사용.\n",
    "    2. 첫 토큰이 없으면, query 내 임의 위치에서 가장 긴 brand 매칭을 탐색.\n",
    "    3. brand가 없으면 model_part = query 전체.\n",
    "\n",
    "    Args:\n",
    "        query: 기기명 질의(원본 또는 정규화).\n",
    "        known_brands: 사운드바 DB의 브랜드 집합(정규화된 값).\n",
    "\n",
    "    Returns:\n",
    "        BrandExtraction\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return BrandExtraction(brand=None, model_part=\"\", original_query=query or \"\")\n",
    "\n",
    "    q = normalize_text(query)\n",
    "    if not q:\n",
    "        return BrandExtraction(brand=None, model_part=\"\", original_query=query)\n",
    "\n",
    "    tokens = q.split()\n",
    "    if not tokens:\n",
    "        return BrandExtraction(brand=None, model_part=q, original_query=query)\n",
    "\n",
    "    # 1) 첫 토큰이 브랜드인 경우\n",
    "    first = tokens[0]\n",
    "    if first in known_brands:\n",
    "        model_part = \" \".join(tokens[1:]).strip()\n",
    "        return BrandExtraction(brand=first, model_part=model_part or q, original_query=query)\n",
    "\n",
    "    # 2) 쿼리 내에서 가장 긴 브랜드 매칭 탐색\n",
    "    matched_brand: Optional[str] = None\n",
    "    matched_len = 0\n",
    "    for b in known_brands:\n",
    "        if not b:\n",
    "            continue\n",
    "        # \"LG\" in \"LG SPEAKER DS80TR\" 또는 \"SONOS\" in \"SONOS ARC\"\n",
    "        if b in q:\n",
    "            if len(b) > matched_len:\n",
    "                matched_len = len(b)\n",
    "                matched_brand = b\n",
    "\n",
    "    if matched_brand:\n",
    "        # 브랜드 제거 후 model_part\n",
    "        rest = q.replace(matched_brand, \"\", 1).strip()\n",
    "        rest = \" \".join(rest.split())\n",
    "        return BrandExtraction(brand=matched_brand, model_part=rest or q, original_query=query)\n",
    "\n",
    "    # 3) 브랜드 없음\n",
    "    return BrandExtraction(brand=None, model_part=q, original_query=query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비사운드바 기기 탐지 (device_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entity 기반 비사운드바 기기(스트리밍/셋톱박스/디코더 등) 탐지 모듈입니다.\n",
    "\n",
    "블랙리스트 방식이 아닌, 질의가 \"사운드바가 아닌 입력 소스\"를 지칭하는지\n",
    "패턴/엔티티 기반으로 판별합니다. 이를 통해 APPLE TV, SKY Q, ORANGE 디코더 등\n",
    "입력 소스명이 사운드바로 잘못 매칭되는 것을 방지합니다.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# 스트리밍/미디어 플레이어 (사운드바 브랜드 아님)\n",
    "_STREAMING_PATTERNS: tuple[str, ...] = (\n",
    "    r\"\\bAPPLE\\s+TV\\b\",\n",
    "    r\"\\bFIRE\\s+TV\\b\",\n",
    "    r\"\\bAMAZON\\s+FIRE\\s+T\\b\",\n",
    "    r\"\\bFIRETV\\b\",\n",
    "    r\"\\bCHROMECAST\\b\",\n",
    "    r\"\\bROKU\\b\",\n",
    "    r\"\\bMAGENTATV\\b\",\n",
    "    r\"\\bTELIA\\s+TV\\b\",\n",
    "    r\"\\bVODAFONE\\s+TV\\b\",\n",
    "    r\"\\bEE\\s+TV\\b\",  # EE TV (BT/영국 통신사 스트리밍)\n",
    "    r\"\\bSHIELD\\b\",  # NVIDIA Shield 셋톱박스\n",
    ")\n",
    "\n",
    "# 셋톱박스/위성 수신기\n",
    "_SETTOP_PATTERNS: tuple[str, ...] = (\n",
    "    r\"\\bSKY\\b\",  # SKY 단독 (SKY Q 외에 SKY, SKY+ 등)\n",
    "    r\"\\bSKY\\s+Q\\b\",\n",
    "    r\"\\bSETTOP\\s+BOX\\b\",\n",
    "    r\"\\bSET\\s+TOP\\s+BOX\\b\",\n",
    "    r\"\\bSTB\\b\",\n",
    "    r\"\\bSF8008\\b\",  # 삼성/기타 셋톱박스 모델\n",
    "    r\"\\bMEDIABOX\\b\",  # 미디어박스 (MEDIABOX 1 등)\n",
    ")\n",
    "\n",
    "# 게임 콘솔 (단독 질의로 사용될 때)\n",
    "_CONSOLE_PATTERNS: tuple[str, ...] = (\n",
    "    r\"\\bXBOX\\b\",\n",
    "    r\"\\bPLAYSTATION\\s+\\d+\\b\",  # PLAYSTATION 4, 5, 6 등 모든 번호\n",
    "    r\"\\bPS\\d+\\b\",  # PS4, PS5, PS6 등 모든 번호\n",
    "    r\"\\bNINTENDO\\b\",  # NINTENDO SWITCH, WII, WII U, DS, 3DS 등 모든 콘솔\n",
    "    r\"\\bCONSOLA\\s+DE\\s+JUEGOS\\b\",\n",
    ")\n",
    "\n",
    "# 일반 미디어 소스 (사운드바 아님) - 디코더/인터넷 박스 포함\n",
    "_GENERIC_SOURCE_PATTERNS: tuple[str, ...] = (\n",
    "    r\"\\bBLURAY\\s+PLAYER\\b\",\n",
    "    r\"\\bBD\\s+PLAYER\\b\",\n",
    "    r\"\\bDVD\\b\",\n",
    "    r\"\\bDVDPLAYER\\b\",\n",
    "    r\"\\bAV\\s+RECEIVER\\b\",\n",
    "    r\"\\bTV\\s+IN\\b\",  # TV IN WOONKAME 등: TV 입력 소스 라벨, 사운드바 아님\n",
    "    # 디코더/인터넷 박스 (다국어) - 통신사(ORANGE 등) 조합은 cosine 유사도로 처리\n",
    "    r\"\\bCODEUR\\b\",\n",
    "    r\"\\bDECODER\\b\",\n",
    "    r\"\\bBOX\\s+INTERNET\\b\",\n",
    "    r\"\\bBO\\s+TIER\\s+D\\b\",\n",
    "    r\"\\bFIBRE\\s+OPTIQUE\\b\",\n",
    ")\n",
    "\n",
    "# 사운드바 DB 브랜드인 경우 무시 (예: SONY BARRA DE SOM)\n",
    "_KNOWN_SOUNDBAR_BRAND_PREFIX = (\n",
    "    \"LG \",\n",
    "    \"SAMSUNG \",\n",
    "    \"SONY \",\n",
    "    \"BOSE \",\n",
    "    \"JBL \",\n",
    "    \"PHILIPS \",\n",
    "    \"DENON \",\n",
    "    \"SONOS \",\n",
    "    \"TCL \",\n",
    "    \"HISENSE \",\n",
    "    \"HARMAN \",\n",
    "    \"YAMAHA \",\n",
    "    \"VIZIO \",\n",
    "    \"POLK \",\n",
    "    \"KLIPSCH \",\n",
    "    \"ROKU \",\n",
    ")\n",
    "\n",
    "_streaming_re = re.compile(\"|\".join(_STREAMING_PATTERNS), re.IGNORECASE)\n",
    "_settop_re = re.compile(\"|\".join(_SETTOP_PATTERNS), re.IGNORECASE)\n",
    "_console_re = re.compile(\"|\".join(_CONSOLE_PATTERNS), re.IGNORECASE)\n",
    "_generic_re = re.compile(\"|\".join(_GENERIC_SOURCE_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def is_non_soundbar_device(query: str) -> bool:\n",
    "    \"\"\"\n",
    "    질의가 사운드바가 아닌 입력 소스(스트리밍/셋톱박스/디코더 등)를 지칭하는지 판별합니다.\n",
    "\n",
    "    블랙리스트가 아닌 패턴 기반입니다. 예:\n",
    "    - \"APPLE TV\" -> True (스트리밍 기기)\n",
    "    - \"SKY Q\" -> True (셋톱박스)\n",
    "    - \"ORANGE FIBRE OPTIQUE BO TIER D CODEUR /BOX INTERNET\" -> True (디코더)\n",
    "    - \"LG S40T\" -> False (사운드바)\n",
    "    - \"SONY SONY BARRA DE SOM\" -> False (사운드바)\n",
    "\n",
    "    Args:\n",
    "        query: 정규화된 질의 문자열\n",
    "\n",
    "    Returns:\n",
    "        비사운드바 기기이면 True, 사운드바일 가능성이 있으면 False\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return False\n",
    "\n",
    "    q = query.upper().strip()\n",
    "\n",
    "    # 사운드바 DB 브랜드로 시작하면 비사운드바 아님 (예: LG S40T, SONY BARRA DE SOM)\n",
    "    for prefix in _KNOWN_SOUNDBAR_BRAND_PREFIX:\n",
    "        if q.startswith(prefix):\n",
    "            return False\n",
    "\n",
    "    # 스트리밍/셋톱/콘솔/일반 미디어(디코더 포함) 패턴 매칭\n",
    "    if _streaming_re.search(q):\n",
    "        return True\n",
    "    if _settop_re.search(q):\n",
    "        return True\n",
    "    if _console_re.search(q):\n",
    "        return True\n",
    "    if _generic_re.search(q):\n",
    "        return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 검색 (embedding_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "정확도 우선 임베딩 기반 검색 모듈입니다.\n",
    "\n",
    "- TF-IDF 프리필터 없이 전체 DB에 대해 임베딩 코사인 유사도로 랭킹\n",
    "- 브랜드 필터(옵션): 추출된 brand로 후보 풀 축소\n",
    "- 속도보다 정확도에 중점\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EmbeddingCandidate:\n",
    "    \"\"\"임베딩 검색 후보 1개를 표현합니다.\"\"\"\n",
    "\n",
    "    canonical: str\n",
    "    score: float\n",
    "    brand: str\n",
    "    model_part: str\n",
    "\n",
    "\n",
    "def _safe_import_sentence_transformers():\n",
    "    \"\"\"sentence-transformers를 지연 로드합니다.\"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    return SentenceTransformer\n",
    "\n",
    "\n",
    "class EmbeddingRetriever:\n",
    "    \"\"\"\n",
    "    임베딩 + 코사인 유사도 기반 검색기입니다.\n",
    "    정확도 우선으로 TF-IDF 프리필터 없이 전체 DB에 대해 검색합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        embedding_model_id: str = \"BAAI/bge-small-en-v1.5\",\n",
    "        normalize_embeddings: bool = True,\n",
    "        top_k: int = 20,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_model_id: sentence-transformers 모델 ID.\n",
    "            normalize_embeddings: L2 정규화하여 cosine을 내적으로 계산.\n",
    "            top_k: 반환 후보 수.\n",
    "        \"\"\"\n",
    "        self.embedding_model_id = embedding_model_id\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self._embedder = None\n",
    "        self._records: list[SoundbarRecord] = []\n",
    "        self._canonicals: list[str] = []\n",
    "        self._brands: list[str] = []\n",
    "        self._model_parts: list[str] = []\n",
    "        self._embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, records: Iterable[SoundbarRecord]) -> None:\n",
    "        \"\"\"\n",
    "        사운드바 DB 레코드로 인덱스를 구축합니다.\n",
    "\n",
    "        Args:\n",
    "            records: SoundbarRecord iterable.\n",
    "        \"\"\"\n",
    "        self._records = list(records)\n",
    "        self._canonicals = [r.canonical for r in self._records if r.canonical]\n",
    "        self._brands = [r.brand or \"\" for r in self._records if r.canonical]\n",
    "        self._model_parts = [\n",
    "            (r.model or r.canonical or \"\").strip() for r in self._records if r.canonical\n",
    "        ]\n",
    "\n",
    "        if not self._canonicals:\n",
    "            raise ValueError(\"인덱싱할 canonical 모델이 없습니다.\")\n",
    "\n",
    "        SentenceTransformer = _safe_import_sentence_transformers()\n",
    "        if SentenceTransformer is None:\n",
    "            raise ImportError(\n",
    "                \"sentence-transformers가 필요합니다. `pip install sentence-transformers`로 설치해 주세요.\"\n",
    "            )\n",
    "\n",
    "        self._embedder = SentenceTransformer(self.embedding_model_id)\n",
    "\n",
    "        # 모델명 검색 정확도 향상: DB의 model_part만 임베딩 (full canonical 대신)\n",
    "        texts_to_embed = [normalize_text(mp) for mp in self._model_parts]\n",
    "        self._embeddings = self._embedder.encode(\n",
    "            texts_to_embed,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            show_progress_bar=True,\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        brand_filter: Optional[str] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "    ) -> list[EmbeddingCandidate]:\n",
    "        \"\"\"\n",
    "        질의에 대해 임베딩 코사인 유사도로 TopK 후보를 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            query: 질의 문자열(기기명 또는 모델 부분).\n",
    "            brand_filter: 지정 시 해당 브랜드 후보만 검색.\n",
    "            top_k: 반환 후보 수(override).\n",
    "\n",
    "        Returns:\n",
    "            EmbeddingCandidate 리스트(내림차순).\n",
    "        \"\"\"\n",
    "        if self._embedder is None or self._embeddings is None:\n",
    "            raise RuntimeError(\"EmbeddingRetriever.fit()을 먼저 호출해야 합니다.\")\n",
    "\n",
    "        q = normalize_text(query)\n",
    "        if not q:\n",
    "            return []\n",
    "\n",
    "        k = int(top_k or self.top_k)\n",
    "        k = max(1, min(k, len(self._canonicals)))\n",
    "\n",
    "        # 브랜드 필터 적용\n",
    "        if brand_filter:\n",
    "            brand_norm = normalize_text(brand_filter)\n",
    "            indices = [i for i in range(len(self._canonicals)) if self._brands[i] == brand_norm]\n",
    "            if not indices:\n",
    "                indices = list(range(len(self._canonicals)))\n",
    "        else:\n",
    "            indices = list(range(len(self._canonicals)))\n",
    "\n",
    "        # 질의 임베딩\n",
    "        q_emb = self._embedder.encode(\n",
    "            [q],\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            show_progress_bar=False,\n",
    "        ).astype(np.float32)[0]\n",
    "\n",
    "        # 코사인 유사도 (정규화된 내적)\n",
    "        cand_emb = self._embeddings[indices]\n",
    "        scores = np.dot(cand_emb, q_emb).astype(np.float64)\n",
    "\n",
    "        # TopK\n",
    "        top_idx = np.argsort(-scores)[:k]\n",
    "        out: list[EmbeddingCandidate] = []\n",
    "        for i in top_idx:\n",
    "            idx = indices[i]\n",
    "            out.append(\n",
    "                EmbeddingCandidate(\n",
    "                    canonical=self._canonicals[idx],\n",
    "                    score=float(scores[i]),\n",
    "                    brand=self._brands[idx],\n",
    "                    model_part=self._model_parts[idx],\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        두 문자열 간 코사인 유사도를 계산합니다.\n",
    "\n",
    "        primary_query와 top1 candidate 간 직접 유사도 검증에 사용합니다.\n",
    "\n",
    "        Args:\n",
    "            text1: 첫 번째 문자열(예: primary_query)\n",
    "            text2: 두 번째 문자열(예: canonical 모델명)\n",
    "\n",
    "        Returns:\n",
    "            코사인 유사도 [0, 1].\n",
    "        \"\"\"\n",
    "        sims = self.compute_similarities_batch(text1, [text2])\n",
    "        return sims[0] if sims else 0.0\n",
    "\n",
    "    def compute_similarities_batch(self, query: str, canonicals: list[str]) -> list[float]:\n",
    "        \"\"\"\n",
    "        질의와 여러 canonical 간 코사인 유사도를 1회 인코딩으로 계산합니다.\n",
    "        (속도 개선: N회 별도 encode 대신 1회 배치 encode)\n",
    "\n",
    "        Args:\n",
    "            query: 질의 문자열\n",
    "            canonicals: canonical 모델명 리스트\n",
    "\n",
    "        Returns:\n",
    "            각 canonical에 대한 유사도 리스트\n",
    "        \"\"\"\n",
    "        if self._embedder is None or not canonicals:\n",
    "            return [1.0] * len(canonicals) if canonicals else []\n",
    "        q = normalize_text(query) or query\n",
    "        if not q:\n",
    "            return [0.0] * len(canonicals)\n",
    "        texts = [q] + [normalize_text(c) or c for c in canonicals]\n",
    "        embs = self._embedder.encode(\n",
    "            texts,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            show_progress_bar=False,\n",
    "        ).astype(np.float32)\n",
    "        q_emb = embs[0]\n",
    "        return [float(np.dot(q_emb, embs[i + 1])) for i in range(len(canonicals))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이브리드 검색 (retrieval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "후보 검색(retrieval) 모듈입니다.\n",
    "\n",
    "구현 목표:\n",
    "- 1차: TF-IDF(문자 n-gram)로 빠르게 TopN 후보를 축소(lexical)\n",
    "- 2차: sentence-transformers 임베딩 cosine 유사도로 TopK 정밀 후보 생성(semantic, 옵션)\n",
    "\n",
    "sentence-transformers가 설치되지 않은 환경에서는 TF-IDF만으로 동작합니다.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetrievedCandidate:\n",
    "    \"\"\"검색으로 얻은 후보 1개를 표현합니다.\"\"\"\n",
    "\n",
    "    canonical: str\n",
    "    score: float\n",
    "    lexical_score: float\n",
    "    semantic_score: Optional[float]\n",
    "\n",
    "\n",
    "def _safe_import_sklearn() -> tuple[object, object, object]:\n",
    "    \"\"\"scikit-learn을 지연 로드합니다(미설치 환경에서 오류 메시지 개선).\"\"\"\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "        from sklearn.metrics.pairwise import linear_kernel  # type: ignore\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        raise ImportError(\n",
    "            \"scikit-learn이 필요합니다. `pip install scikit-learn`로 설치해 주세요.\"\n",
    "        ) from e\n",
    "    return TfidfVectorizer, linear_kernel, np\n",
    "\n",
    "\n",
    "def _safe_import_sentence_transformers():\n",
    "    \"\"\"sentence-transformers를 지연 로드합니다(옵션).\"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    return SentenceTransformer\n",
    "\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    TF-IDF + (옵션) 임베딩 기반 하이브리드 검색기입니다.\n",
    "\n",
    "    Public API:\n",
    "    - `fit(records)`: DB 인덱스 구축\n",
    "    - `retrieve(query)`: TopK 후보 반환\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        char_ngram_range: tuple[int, int] = (3, 5),\n",
    "        min_df: int = 1,\n",
    "        top_n_lexical: int = 200,\n",
    "        top_k: int = 20,\n",
    "        embedding_model_id: Optional[str] = \"BAAI/bge-small-en-v1.5\",\n",
    "        use_embeddings: bool = True,\n",
    "        normalize_embeddings: bool = True,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_ngram_range: TF-IDF char n-gram 범위.\n",
    "            min_df: TF-IDF 최소 문서 빈도.\n",
    "            top_n_lexical: 1차 lexical 후보 수(2차 임베딩 계산 대상).\n",
    "            top_k: 최종 반환 후보 수.\n",
    "            embedding_model_id: sentence-transformers 모델 ID(옵션).\n",
    "            use_embeddings: True이면 임베딩 기반 2차 검색 시도(미설치면 자동 비활성).\n",
    "            normalize_embeddings: 임베딩을 L2 정규화하여 cosine을 내적으로 계산.\n",
    "            random_state: 재현성(일부 모델/연산에서 사용).\n",
    "        \"\"\"\n",
    "        self.char_ngram_range = char_ngram_range\n",
    "        self.min_df = min_df\n",
    "        self.top_n_lexical = top_n_lexical\n",
    "        self.top_k = top_k\n",
    "        self.embedding_model_id = embedding_model_id\n",
    "        self.use_embeddings = use_embeddings\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self._canonicals: list[str] = []\n",
    "        self._tfidf_vectorizer = None\n",
    "        self._tfidf_matrix = None\n",
    "        self._embedder = None\n",
    "        self._embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "    @property\n",
    "    def canonicals(self) -> list[str]:\n",
    "        \"\"\"인덱싱된 canonical 모델 문자열 리스트를 반환합니다.\"\"\"\n",
    "        return list(self._canonicals)\n",
    "\n",
    "    def fit(self, records: Iterable[SoundbarRecord]) -> None:\n",
    "        \"\"\"\n",
    "        사운드바 DB 레코드로 인덱스를 구축합니다.\n",
    "\n",
    "        Args:\n",
    "            records: SoundbarRecord iterable.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: 인덱싱 가능한 canonical이 없을 때.\n",
    "            ImportError: scikit-learn 미설치 시.\n",
    "        \"\"\"\n",
    "        canonicals = [r.canonical for r in records if r.canonical]\n",
    "        canonicals = [normalize_text(c) for c in canonicals]\n",
    "        canonicals = [c for c in canonicals if c]\n",
    "        if not canonicals:\n",
    "            raise ValueError(\"인덱싱할 canonical 모델이 없습니다.\")\n",
    "\n",
    "        TfidfVectorizer, _, _ = _safe_import_sklearn()\n",
    "        self._canonicals = canonicals\n",
    "        self._tfidf_vectorizer = TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=self.char_ngram_range,\n",
    "            min_df=self.min_df,\n",
    "            lowercase=False,\n",
    "        )\n",
    "        self._tfidf_matrix = self._tfidf_vectorizer.fit_transform(self._canonicals)\n",
    "\n",
    "        self._maybe_init_embeddings()\n",
    "        if self._embedder is not None:\n",
    "            self._embeddings = self._encode_texts(self._canonicals)\n",
    "        else:\n",
    "            self._embeddings = None\n",
    "\n",
    "    def _maybe_init_embeddings(self) -> None:\n",
    "        \"\"\"임베딩 모델을 초기화합니다(옵션).\"\"\"\n",
    "        if not self.use_embeddings or not self.embedding_model_id:\n",
    "            self._embedder = None\n",
    "            return\n",
    "        SentenceTransformer = _safe_import_sentence_transformers()\n",
    "        if SentenceTransformer is None:\n",
    "            self._embedder = None\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self._embedder = SentenceTransformer(self.embedding_model_id)\n",
    "        except Exception:\n",
    "            # 다운로드/초기화 실패 시에도 TF-IDF만으로 동작 가능해야 함\n",
    "            self._embedder = None\n",
    "\n",
    "    def _encode_texts(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"텍스트 리스트를 임베딩합니다.\"\"\"\n",
    "        if self._embedder is None:\n",
    "            raise RuntimeError(\"임베딩 모델이 초기화되지 않았습니다.\")\n",
    "        emb = self._embedder.encode(\n",
    "            texts,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        return emb.astype(np.float32, copy=False)\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        top_n_lexical: Optional[int] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "    ) -> list[RetrievedCandidate]:\n",
    "        \"\"\"\n",
    "        주어진 질의 문자열로 후보 TopK를 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query: 질의 문자열(원본 가능, 내부에서 정규화)\n",
    "            top_n_lexical: 1차 후보 수(override)\n",
    "            top_k: 최종 후보 수(override)\n",
    "\n",
    "        Returns:\n",
    "            RetrievedCandidate 리스트(내림차순 정렬).\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: `fit()`이 호출되지 않았을 때.\n",
    "        \"\"\"\n",
    "        if self._tfidf_vectorizer is None or self._tfidf_matrix is None:\n",
    "            raise RuntimeError(\"HybridRetriever.fit()을 먼저 호출해야 합니다.\")\n",
    "\n",
    "        q = normalize_text(query)\n",
    "        if not q:\n",
    "            return []\n",
    "\n",
    "        n_lex = int(top_n_lexical or self.top_n_lexical)\n",
    "        k = int(top_k or self.top_k)\n",
    "        n_lex = max(1, min(n_lex, len(self._canonicals)))\n",
    "        k = max(1, min(k, len(self._canonicals)))\n",
    "\n",
    "        _, linear_kernel, _ = _safe_import_sklearn()\n",
    "        q_vec = self._tfidf_vectorizer.transform([q])\n",
    "        lex_scores = linear_kernel(q_vec, self._tfidf_matrix).ravel()\n",
    "\n",
    "        # 1차: lexical TopN\n",
    "        top_lex_idx = np.argpartition(-lex_scores, kth=min(n_lex - 1, len(lex_scores) - 1))[\n",
    "            :n_lex\n",
    "        ]\n",
    "        top_lex_idx = top_lex_idx[np.argsort(-lex_scores[top_lex_idx])]\n",
    "\n",
    "        # 2차: semantic cosine (옵션)\n",
    "        sem_scores: Optional[np.ndarray] = None\n",
    "        if self._embedder is not None and self._embeddings is not None:\n",
    "            q_emb = self._encode_texts([q])[0]\n",
    "            cand_emb = self._embeddings[top_lex_idx]\n",
    "            # normalize_embeddings=True라면 cosine ~= dot\n",
    "            sem_scores = np.dot(cand_emb, q_emb).astype(np.float32, copy=False)\n",
    "            rerank_idx = np.argsort(-sem_scores)\n",
    "            final_idx = top_lex_idx[rerank_idx][:k]\n",
    "        else:\n",
    "            final_idx = top_lex_idx[:k]\n",
    "\n",
    "        out: list[RetrievedCandidate] = []\n",
    "        for idx in final_idx:\n",
    "            lex = float(lex_scores[idx])\n",
    "            sem = None if sem_scores is None else float(sem_scores[np.where(top_lex_idx == idx)[0][0]])\n",
    "            score = float(sem) if sem is not None else lex\n",
    "            out.append(\n",
    "                RetrievedCandidate(\n",
    "                    canonical=self._canonicals[int(idx)],\n",
    "                    score=score,\n",
    "                    lexical_score=lex,\n",
    "                    semantic_score=sem,\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        두 문자열 간 코사인 유사도를 계산합니다.\n",
    "\n",
    "        primary_query와 top1 candidate 간 직접 유사도 검증에 사용합니다.\n",
    "\n",
    "        Args:\n",
    "            text1: 첫 번째 문자열(예: primary_query)\n",
    "            text2: 두 번째 문자열(예: canonical 모델명)\n",
    "\n",
    "        Returns:\n",
    "            코사인 유사도 [0, 1]. 임베딩 미사용 시 1.0 반환.\n",
    "        \"\"\"\n",
    "        sims = self.compute_similarities_batch(text1, [text2])\n",
    "        return sims[0] if sims else (1.0 if self._embedder is None else 0.0)\n",
    "\n",
    "    def compute_similarities_batch(self, query: str, canonicals: list[str]) -> list[float]:\n",
    "        \"\"\"\n",
    "        질의와 여러 canonical 간 코사인 유사도를 1회 인코딩으로 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            query: 질의 문자열\n",
    "            canonicals: canonical 모델명 리스트\n",
    "\n",
    "        Returns:\n",
    "            각 canonical에 대한 유사도 리스트\n",
    "        \"\"\"\n",
    "        if self._embedder is None or not canonicals:\n",
    "            return [1.0] * len(canonicals) if canonicals else []\n",
    "        q = normalize_text(query) or query\n",
    "        if not q:\n",
    "            return [0.0] * len(canonicals)\n",
    "        texts = [q] + [normalize_text(c) or c for c in canonicals]\n",
    "        embs = self._encode_texts(texts)\n",
    "        q_emb = embs[0]\n",
    "        return [float(np.dot(q_emb, embs[i + 1])) for i in range(len(canonicals))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증 규칙 (verify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNKNOWN 결정 규칙 및 (옵션) 후보 제한 LLM 검증(verifier) 모듈입니다.\n",
    "\n",
    "목표:\n",
    "- 검색(retrieval) 결과가 불확실할 때, \\\"틀릴 바엔 UNKNOWN\\\"을 선택할 수 있도록 함\n",
    "- 점수 threshold, 1-2위 margin, 입력 노이즈(BT 헤드폰 등) 신호를 반영\n",
    "- (옵션) 상위 후보 목록 안에서만 선택하는 closed-book LLM verifier 제공\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Set\n",
    "\n",
    "\n",
    "# 질의 첫 토큰이 아래이면 generic 입력소스(TV IN, AV 등)로 간주. 알려진 브랜드 없을 때 UNKNOWN.\n",
    "_GENERIC_INPUT_FIRST_TOKENS: frozenset[str] = frozenset(\n",
    "    {\"TV\", \"IN\", \"AV\", \"HDMI\", \"USB\", \"OPTICAL\", \"ARC\", \"AUX\", \"DIGITAL\"}\n",
    ")\n",
    "\n",
    "UNKNOWN_LABEL = \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def _get_canonical_model_part(canonical: str) -> str:\n",
    "    \"\"\"canonical 'BRAND MODEL'에서 모델 부분만 추출합니다.\"\"\"\n",
    "    parts = canonical.upper().split()\n",
    "    return \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "\n",
    "\n",
    "def _compact_no_space(s: str) -> str:\n",
    "    \"\"\"공백 제거 후 문자열. ARC ULTRA vs ARCULTRA 매칭용.\"\"\"\n",
    "    return \"\".join(s.upper().split())\n",
    "\n",
    "\n",
    "def _has_lexical_overlap(query: str, canonical: str, min_token_len: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    질의와 canonical 간 의미 있는 어휘 중첩이 있는지 검사합니다.\n",
    "\n",
    "    기기명 블랙리스트 없이, 질의가 사운드바 DB 후보와 무관할 때 UNKNOWN을\n",
    "    선택하는 데 사용합니다. (예: \"SKY Q\" vs \"SONY HTS40R\" -> 중첩 없음 -> UNKNOWN)\n",
    "    ARC ULTRA vs ARCULTRA: 공백 제거 후 동일하면 중첩으로 인정합니다.\n",
    "\n",
    "    Args:\n",
    "        query: 정규화된 질의 문자열\n",
    "        canonical: \"BRAND MODEL\" 형식의 canonical 또는 model_part\n",
    "        min_token_len: 중첩 판별 시 최소 토큰 길이(단일 문자 제외)\n",
    "\n",
    "    Returns:\n",
    "        중첩 있으면 True, 없으면 False\n",
    "    \"\"\"\n",
    "    # 공백 제거 정규화: ARC ULTRA == ARCULTRA\n",
    "    q_compact = _compact_no_space(query)\n",
    "    c_compact = _compact_no_space(canonical)\n",
    "    if q_compact and c_compact and (q_compact in c_compact or c_compact in q_compact):\n",
    "        return True\n",
    "\n",
    "    q_tokens = [t for t in query.upper().split() if len(t) >= min_token_len]\n",
    "    c_tokens = [t for t in canonical.upper().split() if len(t) >= min_token_len]\n",
    "    if not q_tokens or not c_tokens:\n",
    "        return False\n",
    "    for q in q_tokens:\n",
    "        for c in c_tokens:\n",
    "            if q in c or c in q:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _levenshtein_distance(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Levenshtein distance(편집 거리)를 계산합니다.\n",
    "    오타/유사 모델 overlap 판별에 사용합니다.\n",
    "    \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if not a:\n",
    "        return len(b)\n",
    "    if not b:\n",
    "        return len(a)\n",
    "    if len(a) < len(b):\n",
    "        a, b = b, a\n",
    "    prev = list(range(len(b) + 1))\n",
    "    for i, ca in enumerate(a, start=1):\n",
    "        cur = [i]\n",
    "        for j, cb in enumerate(b, start=1):\n",
    "            cur.append(\n",
    "                min(\n",
    "                    cur[j - 1] + 1,\n",
    "                    prev[j] + 1,\n",
    "                    prev[j - 1] + (0 if ca == cb else 1),\n",
    "                )\n",
    "            )\n",
    "        prev = cur\n",
    "    return prev[-1]\n",
    "\n",
    "\n",
    "def _has_typo_overlap(query_part: str, canon_part: str, max_edit_distance: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    오타/유사 모델 overlap을 검사합니다.\n",
    "    LG US20A vs LG S20A, LG NS60TR vs LG S60TR 등 1~2자 차이 허용.\n",
    "\n",
    "    Args:\n",
    "        query_part: 질의의 모델 부분\n",
    "        canon_part: canonical의 모델 부분\n",
    "        max_edit_distance: 허용 최대 편집 거리\n",
    "\n",
    "    Returns:\n",
    "        오타 수준 overlap이면 True\n",
    "    \"\"\"\n",
    "    q = (query_part or \"\").upper().replace(\" \", \"\")\n",
    "    c = (canon_part or \"\").upper().replace(\" \", \"\")\n",
    "    if not q or not c:\n",
    "        return False\n",
    "    # 3자 이상 공통 서브스트링\n",
    "    for ln in range(3, min(len(q), len(c)) + 1):\n",
    "        for i in range(len(q) - ln + 1):\n",
    "            sub = q[i : i + ln]\n",
    "            if sub in c:\n",
    "                return True\n",
    "    # Levenshtein 거리 <= 2 (길이 비슷할 때)\n",
    "    if abs(len(q) - len(c)) <= 2 and _levenshtein_distance(q, c) <= max_edit_distance:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _extract_model_numbers(text: str) -> list[int]:\n",
    "    \"\"\"모델명에서 숫자 시퀀스를 추출합니다. ENCHANT900→[900], S20A→[20].\"\"\"\n",
    "    parts = re.findall(r\"\\d+\", (text or \"\").upper())\n",
    "    return [int(p) for p in parts if p]\n",
    "\n",
    "\n",
    "def _pick_closest_model_candidate(\n",
    "    query: str, candidates: list[RetrievedCandidate]\n",
    ") -> Optional[RetrievedCandidate]:\n",
    "    \"\"\"\n",
    "    small_margin 시 쿼리와 모델 번호가 가장 가까운 후보를 선택합니다.\n",
    "    ENCHANT900 → ENCHANT800(800) 선호, ENCHANT1300(1300) 비선호.\n",
    "    \"\"\"\n",
    "    q_nums = _extract_model_numbers(query)\n",
    "    if not q_nums:\n",
    "        return candidates[0] if candidates else None\n",
    "    q_num = q_nums[-1]  # 주로 마지막 숫자가 모델 번호(900, 800 등)\n",
    "    best: Optional[tuple[RetrievedCandidate, int]] = None\n",
    "    for c in candidates:\n",
    "        canon_part = _get_canonical_model_part(c.canonical) or c.canonical\n",
    "        c_nums = _extract_model_numbers(canon_part)\n",
    "        if not c_nums:\n",
    "            continue\n",
    "        c_num = c_nums[-1]\n",
    "        diff = abs(q_num - c_num)\n",
    "        if best is None or diff < best[1]:\n",
    "            best = (c, diff)\n",
    "    return best[0] if best else (candidates[0] if candidates else None)\n",
    "\n",
    "\n",
    "def _has_model_part_overlap(query: str, canonical: str, min_substring_len: int = 4) -> bool:\n",
    "    \"\"\"\n",
    "    질의와 canonical의 모델 부분 간 거의 일치 수준 중첩이 있는지 검사합니다.\n",
    "\n",
    "    브랜드를 참조하지 못할 때, 모델명이 거의 일치해야만 예측하도록 합니다.\n",
    "    (예: \"SKY Q\" vs \"SONOS ARC\" → model \"ARC\"와 중첩 없음 → UNKNOWN)\n",
    "    (예: \"CINEBAR 11\" vs \"JBL CINEMASB110\" → \"CINE\" 등 4자 이상 중첩 → accept)\n",
    "\n",
    "    Args:\n",
    "        query: 정규화된 질의 문자열\n",
    "        canonical: \"BRAND MODEL\" 형식의 canonical\n",
    "        min_substring_len: 실질적 일치로 인정할 최소 중첩 길이\n",
    "\n",
    "    Returns:\n",
    "        모델 부분과 실질적 중첩 있으면 True\n",
    "    \"\"\"\n",
    "    parts = canonical.upper().split()\n",
    "    model_part = \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "    if not model_part:\n",
    "        return False\n",
    "    q_tokens = [t for t in query.upper().split() if len(t) >= min_substring_len]\n",
    "    if not q_tokens:\n",
    "        return False\n",
    "    for q in q_tokens:\n",
    "        for i in range(len(q) - min_substring_len + 1):\n",
    "            sub = q[i : i + min_substring_len]\n",
    "            if sub in model_part:\n",
    "                return True\n",
    "        if q in model_part or model_part in q:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class VerificationConfig:\n",
    "    \"\"\"검증/결정 로직 설정입니다.\"\"\"\n",
    "\n",
    "    accept_score_threshold: float = 0.52  # 0.55→0.52 (LG NS60TR 등 BT 가중치 완화)\n",
    "    margin_threshold: float = 0.05\n",
    "    prefer_unknown_on_uncertain: bool = True\n",
    "    allow_bt_only_prediction: bool = False\n",
    "    unknown_label: str = UNKNOWN_LABEL\n",
    "    known_brands: Optional[Set[str]] = None  # 사운드바 DB 브랜드 집합, NER 판별용\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Prediction:\n",
    "    \"\"\"\n",
    "    최종 예측 결과입니다.\n",
    "\n",
    "    Attributes:\n",
    "        canonical_model: 예측된 canonical 모델명. UNKNOWN이면 None.\n",
    "        confidence: [0, 1] 범위의 대략적 신뢰도(휴리스틱).\n",
    "        evidence: 근거(입력 질의, 상위 후보/점수 등).\n",
    "    \"\"\"\n",
    "\n",
    "    canonical_model: Optional[str]\n",
    "    confidence: float\n",
    "    evidence: dict[str, Any]\n",
    "\n",
    "\n",
    "class RuleBasedVerifier:\n",
    "    \"\"\"\n",
    "    retrieval 결과를 기반으로 UNKNOWN/모델을 결정하는 규칙 기반 verifier 입니다.\n",
    "\n",
    "    Public API:\n",
    "    - `verify(queries, candidates) -> Prediction`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[VerificationConfig] = None,\n",
    "        llm_verifier: Optional[\"ClosedBookLLMVerifier\"] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: 검증 설정\n",
    "            llm_verifier: (옵션) closed-book LLM verifier\n",
    "        \"\"\"\n",
    "        self.config = config or VerificationConfig()\n",
    "        self.llm_verifier = llm_verifier\n",
    "\n",
    "    def verify(self, queries: list[LogQuery], candidates: list[RetrievedCandidate]) -> Prediction:\n",
    "        \"\"\"\n",
    "        후보 리스트에서 최종 모델 또는 UNKNOWN을 선택합니다.\n",
    "\n",
    "        Args:\n",
    "            queries: 로그에서 추출된 질의 리스트\n",
    "            candidates: retrieval 결과 (score 내림차순 권장)\n",
    "\n",
    "        Returns:\n",
    "            Prediction\n",
    "        \"\"\"\n",
    "        primary = choose_primary_query(queries)\n",
    "        if primary is None:\n",
    "            return Prediction(\n",
    "                canonical_model=None,\n",
    "                confidence=1.0,\n",
    "                evidence={\"reason\": \"no_valid_query\"},\n",
    "            )\n",
    "\n",
    "        if not candidates:\n",
    "            return Prediction(\n",
    "                canonical_model=None,\n",
    "                confidence=1.0,\n",
    "                evidence={\"reason\": \"no_candidates\", \"primary_query\": primary.query},\n",
    "            )\n",
    "\n",
    "        top1 = candidates[0]\n",
    "        top2 = candidates[1] if len(candidates) > 1 else None\n",
    "        margin = (top1.score - top2.score) if top2 is not None else 1.0\n",
    "\n",
    "        # 룰: 브랜드 없이 \"SOUNDBAR\" 또는 \"SOUND BAR\" 단독 → ETC SOUNDBAR\n",
    "        q_compact = _compact_no_space(primary.query)\n",
    "        if q_compact == \"SOUNDBAR\":\n",
    "            return Prediction(\n",
    "                canonical_model=\"ETC SOUNDBAR\",\n",
    "                confidence=1.0,\n",
    "                evidence={\n",
    "                    \"reason\": \"generic_soundbar_rule\",\n",
    "                    \"primary_query\": primary.query,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # Entity-based: display primary(queries[0])이 비사운드바 기기면 UNKNOWN\n",
    "        # APPLE TV, SKY Q, ORANGE 디코더 등 입력 소스는 사운드바가 아님\n",
    "        if queries and is_non_soundbar_device(queries[0].query):\n",
    "            return Prediction(\n",
    "                canonical_model=None,\n",
    "                confidence=0.85,\n",
    "                evidence={\n",
    "                    \"reason\": \"non_soundbar_device\",\n",
    "                    \"primary_query\": queries[0].query,\n",
    "                    \"top1\": top1.__dict__,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # NER 기반: 질의에서 브랜드 추출 (known_brands = 사운드바 DB 브랜드 집합)\n",
    "        known_brands = self.config.known_brands or set()\n",
    "        extraction = (\n",
    "            extract_brand_from_query(primary.query, known_brands) if known_brands else None\n",
    "        )\n",
    "        query_has_known_brand = extraction is not None and extraction.brand is not None\n",
    "\n",
    "        # 브랜드 없음: no_known_brand로 UNKNOWN 반환하지 않음.\n",
    "        # 모델명 + 사운드바 DB 간 코사인 유사도로 매칭(agent의 cosine gate에서 처리, 공백 제거 적용)\n",
    "\n",
    "        soundbar_hint_re = re.compile(r\"\\b(SOUNDBAR|SOUND\\s*BAR)\\b\")\n",
    "        # overlap 검사 제거: cosine 유사도(0.70~0.85)로만 accept/reject 결정 (agent cosine gate)\n",
    "\n",
    "        # \"브랜드 + SOUNDBAR\"처럼 모델이 부정확한 입력은, 브랜드의 generic soundbar 엔트리를 우대\n",
    "        # - 예: \"LG SOUND BAR\" -> \"LG LGSOUNDBAR\" (DB에 존재하는 경우)\n",
    "        generic_soundbar_relax_threshold = 0.45\n",
    "        if soundbar_hint_re.search(primary.query):\n",
    "            # query의 첫 토큰을 브랜드로 가정 (예: \"LG LG SOUND BAR\"의 첫 토큰 \"LG\")\n",
    "            parts = primary.query.split()\n",
    "            brand = parts[0] if parts else \"\"\n",
    "            if brand and top1.canonical.startswith(f\"{brand} \"):\n",
    "                # canonical 안에 \"SOUNDBAR\"가 포함된 generic 모델이면 낮은 threshold로 accept\n",
    "                if \"SOUNDBAR\" in top1.canonical and top1.score >= generic_soundbar_relax_threshold:\n",
    "                    return Prediction(\n",
    "                        canonical_model=top1.canonical,\n",
    "                        confidence=float(min(1.0, max(0.0, top1.score + 0.25))),\n",
    "                        evidence={\n",
    "                            \"reason\": \"generic_soundbar_accept\",\n",
    "                            \"primary_query\": primary.query,\n",
    "                            \"top1\": top1.__dict__,\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "        # BT만 있는 경우(헤드폰/이어폰 가능성이 높음) 보수적으로 UNKNOWN\n",
    "        if (\n",
    "            primary.source == \"NAME_BT\"\n",
    "            and bt_looks_like_non_soundbar(primary.query)\n",
    "            and not self.config.allow_bt_only_prediction\n",
    "        ):\n",
    "            # 매우 높은 점수면 예외 허용\n",
    "            if top1.score < max(self.config.accept_score_threshold, 0.75):\n",
    "                return Prediction(\n",
    "                    canonical_model=None,\n",
    "                    confidence=0.9,\n",
    "                    evidence={\n",
    "                        \"reason\": \"bt_noise\",\n",
    "                        \"primary_query\": primary.query,\n",
    "                        \"top1\": top1.__dict__,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        # threshold 미만이면 UNKNOWN\n",
    "        if top1.score < self.config.accept_score_threshold:\n",
    "            return Prediction(\n",
    "                canonical_model=None,\n",
    "                confidence=1.0 - min(1.0, top1.score),\n",
    "                evidence={\n",
    "                    \"reason\": \"below_threshold\",\n",
    "                    \"primary_query\": primary.query,\n",
    "                    \"top1\": top1.__dict__,\n",
    "                    \"threshold\": self.config.accept_score_threshold,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # margin이 작으면 불확실 -> (옵션) LLM 검증\n",
    "        # overlap 기반 margin 완화 제거, cosine gate에서 처리\n",
    "        margin_thresh = self.config.margin_threshold\n",
    "        if top2 is not None and margin < margin_thresh:\n",
    "            if self.llm_verifier is not None:\n",
    "                selected = self.llm_verifier.select(primary.query, candidates[:5])\n",
    "                if selected is None:\n",
    "                    return Prediction(\n",
    "                        canonical_model=None,\n",
    "                        confidence=0.6,\n",
    "                        evidence={\n",
    "                            \"reason\": \"llm_unknown\",\n",
    "                            \"primary_query\": primary.query,\n",
    "                            \"top5\": [c.__dict__ for c in candidates[:5]],\n",
    "                        },\n",
    "                    )\n",
    "                return Prediction(\n",
    "                    canonical_model=selected,\n",
    "                    confidence=0.65,\n",
    "                    evidence={\n",
    "                        \"reason\": \"llm_selected\",\n",
    "                        \"primary_query\": primary.query,\n",
    "                        \"selected\": selected,\n",
    "                        \"top5\": [c.__dict__ for c in candidates[:5]],\n",
    "                    },\n",
    "                )\n",
    "\n",
    "            if self.config.prefer_unknown_on_uncertain:\n",
    "                # cosine 기반: 모델 번호가 쿼리와 가장 가까운 후보 선택 (ENCHANT900→ENCHANT800 등)\n",
    "                closest = _pick_closest_model_candidate(primary.query, candidates[:5])\n",
    "                if closest is not None:\n",
    "                    return Prediction(\n",
    "                        canonical_model=closest.canonical,\n",
    "                        confidence=0.6,\n",
    "                        evidence={\n",
    "                            \"reason\": \"small_margin_closest_model\",\n",
    "                            \"primary_query\": primary.query,\n",
    "                            \"selected\": closest.__dict__,\n",
    "                            \"margin\": margin,\n",
    "                            \"margin_threshold\": self.config.margin_threshold,\n",
    "                        },\n",
    "                    )\n",
    "                return Prediction(\n",
    "                    canonical_model=None,\n",
    "                    confidence=0.55,\n",
    "                    evidence={\n",
    "                        \"reason\": \"small_margin\",\n",
    "                        \"primary_query\": primary.query,\n",
    "                        \"top1\": top1.__dict__,\n",
    "                        \"top2\": top2.__dict__,\n",
    "                        \"margin\": margin,\n",
    "                        \"margin_threshold\": self.config.margin_threshold,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        # 기본: top1 채택\n",
    "        confidence = float(min(1.0, max(0.0, top1.score)))\n",
    "        return Prediction(\n",
    "            canonical_model=top1.canonical,\n",
    "            confidence=confidence,\n",
    "            evidence={\n",
    "                \"reason\": \"accepted\",\n",
    "                \"primary_query\": primary.query,\n",
    "                \"top1\": top1.__dict__,\n",
    "                \"margin\": margin,\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "class ClosedBookLLMVerifier:\n",
    "    \"\"\"\n",
    "    후보 목록(topN) 안에서만 답하도록 강제하는 LLM verifier 입니다.\n",
    "\n",
    "    이 구현은 transformers 기반 로컬 모델을 가정하며, 미설치/미로드 환경에서는\n",
    "    ImportError/RuntimeError를 발생시킵니다.\n",
    "\n",
    "    Public API:\n",
    "    - `select(observed, candidates) -> canonical|None`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        *,\n",
    "        device: str = \"auto\",\n",
    "        max_new_tokens: int = 64,\n",
    "        temperature: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_id: HuggingFace 모델 ID (예: \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "            device: \"auto\" 또는 \"cpu\"/\"cuda\" 등\n",
    "            max_new_tokens: 생성 토큰 수\n",
    "            temperature: 샘플링 온도(0이면 거의 결정적)\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.device = device\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self._generator = self._init_generator()\n",
    "\n",
    "    def _init_generator(self):\n",
    "        \"\"\"transformers text-generation 파이프라인을 초기화합니다.\"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline  # type: ignore\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            raise ImportError(\n",
    "                \"transformers가 필요합니다. `pip install transformers`로 설치해 주세요.\"\n",
    "            ) from e\n",
    "\n",
    "        # device=\"auto\" 지원 여부는 버전에 따라 다를 수 있어 예외를 허용\n",
    "        try:\n",
    "            return pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_id,\n",
    "                device_map=self.device,\n",
    "            )\n",
    "        except Exception:\n",
    "            # fallback: device_map 없이 시도\n",
    "            return pipeline(\"text-generation\", model=self.model_id)\n",
    "\n",
    "    def select(\n",
    "        self, observed: str, candidates: list[RetrievedCandidate], unknown_label: str = UNKNOWN_LABEL\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        관측 문자열과 후보 목록을 입력으로 받아, 후보 중 하나 또는 UNKNOWN을 선택합니다.\n",
    "\n",
    "        Args:\n",
    "            observed: 관측된 기기명/질의 문자열\n",
    "            candidates: 상위 후보 리스트(보통 Top5)\n",
    "            unknown_label: UNKNOWN 라벨 문자열\n",
    "\n",
    "        Returns:\n",
    "            선택된 canonical 문자열 또는 None(UNKNOWN)\n",
    "        \"\"\"\n",
    "        if not candidates:\n",
    "            return None\n",
    "\n",
    "        # closed-book 강제를 위해 후보 목록을 명시하고, JSON으로만 답하게 지시\n",
    "        options = [c.canonical for c in candidates]\n",
    "        prompt = self._build_prompt(observed, options, unknown_label=unknown_label)\n",
    "        out = self._generator(\n",
    "            prompt,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=self.temperature > 0.0,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        text = self._extract_generated_text(out)\n",
    "        selected = self._parse_selection(text, options, unknown_label=unknown_label)\n",
    "        return selected\n",
    "\n",
    "    def _build_prompt(self, observed: str, options: list[str], unknown_label: str) -> str:\n",
    "        \"\"\"LLM 프롬프트를 생성합니다.\"\"\"\n",
    "        payload = {\n",
    "            \"observed\": observed,\n",
    "            \"options\": options,\n",
    "            \"rules\": [\n",
    "                \"정답은 options 중 하나를 그대로 선택하거나 UNKNOWN을 선택한다.\",\n",
    "                \"options에 없는 문자열을 생성하지 않는다.\",\n",
    "                \"출력은 JSON 한 줄만 반환한다: {\\\"choice\\\": \\\"...\\\", \\\"reason\\\": \\\"...\\\"}\",\n",
    "            ],\n",
    "        }\n",
    "        return (\n",
    "            \"다음은 사운드바 모델 매핑 문제이다.\\n\"\n",
    "            f\"{json.dumps(payload, ensure_ascii=False)}\\n\"\n",
    "            \"JSON으로만 답해라.\\n\"\n",
    "        )\n",
    "\n",
    "    def _extract_generated_text(self, out: Any) -> str:\n",
    "        \"\"\"transformers pipeline 출력에서 생성 텍스트를 추출합니다.\"\"\"\n",
    "        if isinstance(out, list) and out:\n",
    "            item = out[0]\n",
    "            if isinstance(item, dict) and \"generated_text\" in item:\n",
    "                return str(item[\"generated_text\"])\n",
    "        return str(out)\n",
    "\n",
    "    def _parse_selection(\n",
    "        self, generated_text: str, options: list[str], unknown_label: str\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        생성 결과에서 choice를 파싱합니다.\n",
    "\n",
    "        JSON 파싱 실패 시, options에 대한 문자열 포함 여부로 보조 추정합니다.\n",
    "        \"\"\"\n",
    "        # 가장 마지막 JSON 객체를 찾기 위해 간단히 {...} 블록을 스캔\n",
    "        m = re.findall(r\"\\{[\\s\\S]*?\\}\", generated_text)\n",
    "        for blob in reversed(m):\n",
    "            try:\n",
    "                obj = json.loads(blob)\n",
    "                choice = str(obj.get(\"choice\", \"\")).strip()\n",
    "                if not choice or choice.upper() == unknown_label:\n",
    "                    return None\n",
    "                if choice in options:\n",
    "                    return choice\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # fallback: 옵션 문자열이 그대로 포함되어 있으면 그 중 첫 매칭 반환\n",
    "        for opt in options:\n",
    "            if opt in generated_text:\n",
    "                return opt\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 (load_hdmi_bt_log_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CsvLoadResult:\n",
    "    path: Path\n",
    "    df: pd.DataFrame\n",
    "\n",
    "def load_hdmi_bt_log_csv(path: Path, encoding: str = \"utf-8\") -> CsvLoadResult:\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=encoding)\n",
    "    except FileNotFoundError:\n",
    "        raise\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise OSError(f\"CSV 인코딩 오류: {path}\") from e\n",
    "    except Exception as e:\n",
    "        raise OSError(f\"CSV 로드 실패: {path}\") from e\n",
    "    required_cols = [\"NAME_BT\", \"NAME1\", \"BRAND1\", \"NAME2\", \"BRAND2\", \"NAME3\", \"BRAND3\", \"NAME4\", \"BRAND4\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"필수 컬럼이 없습니다: {missing}\")\n",
    "    return CsvLoadResult(path=path, df=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent (SoundbarModelAgent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "사운드바 모델 매핑 Agent(오케스트레이션) 모듈입니다.\n",
    "\n",
    "파이프라인:\n",
    "1) 사운드바 DB 로드/정규화\n",
    "2) 로그 행에서 후보 질의 생성\n",
    "3) 하이브리드 검색(TF-IDF + 임베딩)으로 후보 TopK 생성\n",
    "4) verifier로 UNKNOWN/모델 결정\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "    LogQuery,\n",
    "    TypedQuery,\n",
    "    build_log_queries_from_row,\n",
    "    build_typed_queries_from_row,\n",
    "    choose_primary_query,\n",
    ")\n",
    "    Prediction,\n",
    "    RuleBasedVerifier,\n",
    "    VerificationConfig,\n",
    "    _get_canonical_model_part,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AgentConfig:\n",
    "    \"\"\"Agent 구성/하이퍼파라미터 설정입니다.\"\"\"\n",
    "\n",
    "    top_n_lexical: int = 200\n",
    "    top_k: int = 20\n",
    "    embedding_model_id: Optional[str] = \"BAAI/bge-small-en-v1.5\"\n",
    "    use_embeddings: bool = True\n",
    "    include_bt: bool = True\n",
    "    accept_score_threshold: float = 0.52  # 0.55→0.52 (LG NS60TR 등 BT 가중치 완화)\n",
    "    margin_threshold: float = 0.05\n",
    "    accuracy_mode: bool = False  # True: NER+임베딩 전용, 정확도 우선\n",
    "    min_cosine_similarity: Optional[float] = 0.85  # primary_query vs top1 직접 유사도. None이면 비활성\n",
    "    cosine_relaxed_with_overlap: float = 0.70  # overlap 있을 때 fallback 검사용 완화 임계치 (0.65→0.70, 오탐 감소)\n",
    "    cosine_gate_max_candidates: int = 5  # cosine gate에서 검사하는 최대 후보 수\n",
    "    min_lexical_similarity: Optional[float] = 0.3  # 모델 파트 렉시컬 유사도 미만이면 UNKNOWN. None이면 비활성\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PredictionResult:\n",
    "    \"\"\"\n",
    "    Agent의 단일 row 예측 결과입니다.\n",
    "\n",
    "    Attributes:\n",
    "        row_id: 원본 데이터 행 식별자(없으면 None)\n",
    "        predicted: 예측 canonical 모델명(UNKNOWN이면 None)\n",
    "        confidence: 신뢰도\n",
    "        primary_query: 대표 질의(정규화 문자열)\n",
    "        candidates: 상위 후보 리스트(내림차순)\n",
    "        evidence: verifier에서 반환한 근거 dict\n",
    "    \"\"\"\n",
    "\n",
    "    row_id: Optional[int]\n",
    "    predicted: Optional[str]\n",
    "    confidence: float\n",
    "    primary_query: str\n",
    "    candidates: list[RetrievedCandidate]\n",
    "    evidence: dict[str, Any]\n",
    "\n",
    "\n",
    "class SoundbarModelAgent:\n",
    "    \"\"\"\n",
    "    사운드바 모델 매핑 Agent 입니다.\n",
    "\n",
    "    Public API:\n",
    "    - `predict_row(row_dict, row_id=None) -> PredictionResult`\n",
    "    - `batch_predict(df) -> pd.DataFrame`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, soundbar_list_py: Path, config: Optional[AgentConfig] = None) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            soundbar_list_py: `soundbar_list.py` 경로\n",
    "            config: AgentConfig\n",
    "        \"\"\"\n",
    "        self.soundbar_list_py = soundbar_list_py\n",
    "        self.config = config or AgentConfig()\n",
    "\n",
    "        self._records: list[SoundbarRecord] = []\n",
    "        self._brand_set: set[str] = set()\n",
    "        if self.config.accuracy_mode:\n",
    "            self._embedding_retriever = EmbeddingRetriever(\n",
    "                embedding_model_id=self.config.embedding_model_id or \"BAAI/bge-small-en-v1.5\",\n",
    "                top_k=self.config.top_k,\n",
    "            )\n",
    "            self._hybrid_retriever: Optional[HybridRetriever] = None\n",
    "        else:\n",
    "            self._embedding_retriever = None\n",
    "            self._hybrid_retriever = HybridRetriever(\n",
    "                top_n_lexical=self.config.top_n_lexical,\n",
    "                top_k=self.config.top_k,\n",
    "                embedding_model_id=self.config.embedding_model_id,\n",
    "                use_embeddings=self.config.use_embeddings,\n",
    "            )\n",
    "        self._verifier: Optional[RuleBasedVerifier] = None\n",
    "\n",
    "        self._load_and_build()\n",
    "\n",
    "    def _load_and_build(self) -> None:\n",
    "        \"\"\"사운드바 DB 로드 및 인덱스를 구축합니다.\"\"\"\n",
    "        self._records = load_soundbar_db_from_py(self.soundbar_list_py)\n",
    "        self._brand_set = get_brand_set(self._records)\n",
    "        if self._embedding_retriever is not None:\n",
    "            self._embedding_retriever.fit(self._records)\n",
    "        elif self._hybrid_retriever is not None:\n",
    "            self._hybrid_retriever.fit(self._records)\n",
    "\n",
    "        self._verifier = RuleBasedVerifier(\n",
    "            config=VerificationConfig(\n",
    "                accept_score_threshold=self.config.accept_score_threshold,\n",
    "                margin_threshold=self.config.margin_threshold,\n",
    "                known_brands=self._brand_set,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def brand_set(self) -> set[str]:\n",
    "        \"\"\"DB로부터 추출된 브랜드 집합을 반환합니다.\"\"\"\n",
    "        return set(self._brand_set)\n",
    "\n",
    "    def _embedding_cands_to_retrieved(\n",
    "        self, cands: list, weights: float = 1.0\n",
    "    ) -> list[RetrievedCandidate]:\n",
    "        \"\"\"EmbeddingCandidate를 RetrievedCandidate로 변환합니다.\"\"\"\n",
    "        out: list[RetrievedCandidate] = []\n",
    "        for c in cands:\n",
    "            wscore = float(c.score) * float(weights)\n",
    "            out.append(\n",
    "                RetrievedCandidate(\n",
    "                    canonical=c.canonical,\n",
    "                    score=wscore,\n",
    "                    lexical_score=0.0,\n",
    "                    semantic_score=c.score,\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def _retrieve_accuracy(self, queries: list[LogQuery]) -> list[RetrievedCandidate]:\n",
    "        \"\"\"정확도 모드: NER로 브랜드 추출 후 임베딩 검색.\"\"\"\n",
    "        best: dict[str, RetrievedCandidate] = {}\n",
    "        best_weighted: dict[str, float] = {}\n",
    "\n",
    "        for q in queries:\n",
    "            extraction = extract_brand_from_query(q.query, self._brand_set)\n",
    "            search_query = extraction.model_part or extraction.original_query\n",
    "            if not search_query.strip():\n",
    "                continue\n",
    "            cands = self._embedding_retriever.retrieve(\n",
    "                search_query,\n",
    "                brand_filter=extraction.brand,\n",
    "                top_k=self.config.top_k,\n",
    "            )\n",
    "            for c in cands:\n",
    "                wscore = float(c.score) * float(q.weight)\n",
    "                prev = best_weighted.get(c.canonical)\n",
    "                if prev is None or wscore > prev:\n",
    "                    best_weighted[c.canonical] = wscore\n",
    "                    best[c.canonical] = RetrievedCandidate(\n",
    "                        canonical=c.canonical,\n",
    "                        score=wscore,\n",
    "                        lexical_score=0.0,\n",
    "                        semantic_score=c.score,\n",
    "                    )\n",
    "\n",
    "        merged = list(best.values())\n",
    "        merged.sort(key=lambda x: x.score, reverse=True)\n",
    "        return merged\n",
    "\n",
    "    def _retrieve_for_queries(self, queries: list[LogQuery]) -> list[RetrievedCandidate]:\n",
    "        \"\"\"\n",
    "        여러 query에 대해 retrieval을 수행하고 결과를 통합합니다.\n",
    "\n",
    "        통합 방식:\n",
    "        - canonical별로 최고(weighted) 점수를 유지\n",
    "        - 최종 점수 = candidate.score * query.weight\n",
    "        \"\"\"\n",
    "        if self._embedding_retriever is not None:\n",
    "            return self._retrieve_accuracy(queries)\n",
    "\n",
    "        best: dict[str, RetrievedCandidate] = {}\n",
    "        best_weighted: dict[str, float] = {}\n",
    "\n",
    "        for q in queries:\n",
    "            cands = self._hybrid_retriever.retrieve(\n",
    "                q.query,\n",
    "                top_n_lexical=self.config.top_n_lexical,\n",
    "                top_k=self.config.top_k,\n",
    "            )\n",
    "            for c in cands:\n",
    "                wscore = float(c.score) * float(q.weight)\n",
    "                prev = best_weighted.get(c.canonical)\n",
    "                if prev is None or wscore > prev:\n",
    "                    best_weighted[c.canonical] = wscore\n",
    "                    best[c.canonical] = RetrievedCandidate(\n",
    "                        canonical=c.canonical,\n",
    "                        score=wscore,\n",
    "                        lexical_score=float(c.lexical_score),\n",
    "                        semantic_score=c.semantic_score,\n",
    "                    )\n",
    "\n",
    "        merged = list(best.values())\n",
    "        merged.sort(key=lambda x: x.score, reverse=True)\n",
    "        return merged\n",
    "\n",
    "    def _apply_cosine_gate(\n",
    "        self,\n",
    "        pred: Prediction,\n",
    "        primary_query: str,\n",
    "        candidates: list[RetrievedCandidate],\n",
    "    ) -> Prediction:\n",
    "        \"\"\"\n",
    "        UNKNOWN이 아닌 예측에 대해 2단계 검증을 수행합니다:\n",
    "        1) 어휘/서브스트링 중첩: primary_query와 예측 모델 간 중첩이 없으면 UNKNOWN\n",
    "        2) 코사인 유사도: primary_query vs canonical 간 유사도가 임계치 미만이면 UNKNOWN\n",
    "        3) top1이 거절되면 overlap+cosine 통과하는 다른 후보로 fallback (ARC ULTRA → ARCULTRA 등)\n",
    "\n",
    "        사운드바가 아닌 기기(SKY, EE TV 등) 오탐을 줄이며, 정답이 top2~k에 있는 경우도 반영합니다.\n",
    "        \"\"\"\n",
    "        # below_threshold/small_margin: top1 실패 시 cosine 통과하는 다른 후보 시도\n",
    "        if pred.canonical_model is None and candidates and pred.evidence.get(\"reason\") in (\n",
    "            \"below_threshold\",\n",
    "            \"small_margin\",\n",
    "        ):\n",
    "            pseudo = Prediction(\n",
    "                canonical_model=candidates[0].canonical,\n",
    "                confidence=pred.confidence,\n",
    "                evidence=pred.evidence,\n",
    "            )\n",
    "            pred = self._apply_cosine_gate(pseudo, primary_query, candidates)\n",
    "\n",
    "        if pred.canonical_model is None:\n",
    "            return pred\n",
    "\n",
    "        # generic_soundbar_rule(SOUNDBAR→ETC SOUNDBAR): 규칙 기반이므로 cosine gate로 덮어쓰지 않음\n",
    "        if pred.evidence.get(\"reason\") == \"generic_soundbar_rule\":\n",
    "            return pred\n",
    "\n",
    "        retriever = self._embedding_retriever or self._hybrid_retriever\n",
    "        threshold = self.config.min_cosine_similarity\n",
    "        min_lex = getattr(self.config, \"min_lexical_similarity\", None)\n",
    "\n",
    "        # 질의 모델 파트 추출 (SONOS ARC ULTRA → ARCULTRA, canonical 모델 파트와 비교)\n",
    "        extraction = extract_brand_from_query(primary_query, self._brand_set)\n",
    "        q_model_part = (extraction.model_part or extraction.original_query or \"\").strip()\n",
    "        q_compact = \"\".join(q_model_part.upper().split()) if q_model_part else \"\"\n",
    "\n",
    "        # cosine 통과하는 후보 중 선택 (상위 N개 검사, 통과 시 렉시컬 재순위화)\n",
    "        max_n = getattr(self.config, \"cosine_gate_max_candidates\", 5) or 5\n",
    "        candidates_to_check = candidates[:max_n]\n",
    "\n",
    "        # 배치 유사도 계산 (1~2회 encode로 N개 후보 처리, 속도 개선)\n",
    "        passing: list[tuple[RetrievedCandidate, float]] = []\n",
    "        if threshold is None or retriever is None:\n",
    "            passing = [(c, 1.0) for c in candidates_to_check]\n",
    "        else:\n",
    "            canonicals_to_check = [c.canonical for c in candidates_to_check]\n",
    "            try:\n",
    "                batch_sims = (\n",
    "                    retriever.compute_similarities_batch(primary_query, canonicals_to_check)\n",
    "                    if hasattr(retriever, \"compute_similarities_batch\")\n",
    "                    else [retriever.compute_similarity(primary_query, canon) for canon in canonicals_to_check]\n",
    "                )\n",
    "                if q_compact and hasattr(retriever, \"compute_similarities_batch\"):\n",
    "                    mp_compacts = [\n",
    "                        \"\".join((_get_canonical_model_part(c.canonical) or c.canonical or \"\").upper().split())\n",
    "                        for c in candidates_to_check\n",
    "                    ]\n",
    "                    compact_sims = retriever.compute_similarities_batch(q_compact, mp_compacts)\n",
    "                    batch_sims = [max(s, cs) for s, cs in zip(batch_sims, compact_sims)]\n",
    "                for c, sim in zip(candidates_to_check, batch_sims):\n",
    "                    mp_compact = \"\".join((_get_canonical_model_part(c.canonical) or c.canonical or \"\").upper().split())\n",
    "                    if q_compact and mp_compact and q_compact == mp_compact:\n",
    "                        passing.append((c, 1.0))\n",
    "                        continue\n",
    "                    if sim >= threshold:\n",
    "                        passing.append((c, sim))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 렉시컬 재순위화: cosine 동일 시 모델 파트 문자열 유사도로 우선순위\n",
    "        def _lex_ratio(cand: RetrievedCandidate) -> float:\n",
    "            mp = \"\".join(\n",
    "                (_get_canonical_model_part(cand.canonical) or cand.canonical or \"\").upper().split()\n",
    "            )\n",
    "            return SequenceMatcher(None, q_compact, mp).ratio() if q_compact and mp else 0.0\n",
    "\n",
    "        passing_with_lex = [(c, sim, _lex_ratio(c)) for c, sim in passing]\n",
    "        passing_with_lex.sort(key=lambda x: (x[1], x[2]), reverse=True)  # cosine desc, lex desc\n",
    "        best = (passing_with_lex[0][0], passing_with_lex[0][1], passing_with_lex[0][2]) if passing_with_lex else None\n",
    "\n",
    "        # 렉시컬 거부 게이트: 모델 파트 유사도가 매우 낮으면 UNKNOWN (YAMAHA RX-A1040 vs SRB40 등)\n",
    "        if best is not None and min_lex is not None and best[2] < min_lex:\n",
    "            top1 = candidates[0] if candidates else None\n",
    "            return Prediction(\n",
    "                canonical_model=None,\n",
    "                confidence=0.85,\n",
    "                evidence={\n",
    "                    \"reason\": \"low_lexical_similarity\",\n",
    "                    \"primary_query\": primary_query,\n",
    "                    \"top1\": top1.__dict__ if top1 else None,\n",
    "                    \"lexical_ratio\": round(best[2], 4),\n",
    "                    \"threshold\": min_lex,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        if best is not None:\n",
    "            c, sim = best[0], best[1]\n",
    "            return Prediction(\n",
    "                canonical_model=c.canonical,\n",
    "                confidence=float(min(1.0, max(0.0, c.score))),\n",
    "                evidence={\n",
    "                    \"reason\": \"cosine_gate_accepted\",\n",
    "                    \"primary_query\": primary_query,\n",
    "                    \"selected\": c.__dict__,\n",
    "                    \"cosine_similarity\": round(sim, 4) if sim > 0 else None,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # 모든 후보가 통과 실패 → UNKNOWN (cosine 기반)\n",
    "        top1 = candidates[0] if candidates else None\n",
    "        reason = \"low_cosine_similarity\"\n",
    "        first_sim = 0.0\n",
    "        if top1 and retriever and threshold is not None:\n",
    "            try:\n",
    "                first_sim = retriever.compute_similarity(primary_query, top1.canonical)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return Prediction(\n",
    "            canonical_model=None,\n",
    "            confidence=0.85,\n",
    "            evidence={\n",
    "                \"reason\": reason,\n",
    "                \"primary_query\": primary_query,\n",
    "                \"top1\": top1.__dict__ if top1 else None,\n",
    "                \"cosine_similarity\": round(first_sim, 4) if first_sim > 0 else None,\n",
    "                \"threshold\": threshold,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def predict_for_query(self, typed: TypedQuery) -> PredictionResult:\n",
    "        \"\"\"\n",
    "        단일 TypedQuery에 대해 사운드바 모델을 예측합니다.\n",
    "\n",
    "        Args:\n",
    "            typed: TypedQuery (type_, source, query, raw, weight)\n",
    "\n",
    "        Returns:\n",
    "            PredictionResult\n",
    "        \"\"\"\n",
    "        log_q = LogQuery(query=typed.query, raw=typed.raw, source=typed.source, weight=typed.weight)\n",
    "        if self._embedding_retriever is not None:\n",
    "            extraction = extract_brand_from_query(typed.query, self._brand_set)\n",
    "            search_query = extraction.model_part or extraction.original_query\n",
    "            cands = self._embedding_retriever.retrieve(\n",
    "                search_query,\n",
    "                brand_filter=extraction.brand,\n",
    "                top_k=self.config.top_k,\n",
    "            )\n",
    "            weighted = self._embedding_cands_to_retrieved(cands, typed.weight)\n",
    "        else:\n",
    "            cands = self._hybrid_retriever.retrieve(\n",
    "                typed.query,\n",
    "                top_n_lexical=self.config.top_n_lexical,\n",
    "                top_k=self.config.top_k,\n",
    "            )\n",
    "            weighted = [\n",
    "                RetrievedCandidate(\n",
    "                    canonical=c.canonical,\n",
    "                    score=float(c.score) * float(typed.weight),\n",
    "                    lexical_score=float(c.lexical_score) if c.lexical_score is not None else 0.0,\n",
    "                    semantic_score=c.semantic_score,\n",
    "                )\n",
    "                for c in cands\n",
    "            ]\n",
    "        pred: Prediction = self._verifier.verify([log_q], weighted)\n",
    "        # small_margin이면 verifier가 불확실한 것 → cosine gate로 후보 재검토\n",
    "        if (\n",
    "            pred.canonical_model is None\n",
    "            and pred.evidence.get(\"reason\") == \"small_margin\"\n",
    "            and weighted\n",
    "        ):\n",
    "            pseudo = Prediction(\n",
    "                canonical_model=weighted[0].canonical,\n",
    "                confidence=pred.confidence,\n",
    "                evidence=pred.evidence,\n",
    "            )\n",
    "            pred = self._apply_cosine_gate(pseudo, typed.query, weighted)\n",
    "        else:\n",
    "            pred = self._apply_cosine_gate(pred, typed.query, weighted)\n",
    "        return PredictionResult(\n",
    "            row_id=None,\n",
    "            predicted=pred.canonical_model,\n",
    "            confidence=float(pred.confidence),\n",
    "            primary_query=typed.raw,\n",
    "            candidates=weighted,\n",
    "            evidence=dict(pred.evidence),\n",
    "        )\n",
    "\n",
    "    def predict_row(self, row: dict[str, Any], *, row_id: Optional[int] = None) -> PredictionResult:\n",
    "        \"\"\"\n",
    "        단일 로그 row(dict)에 대해 사운드바 모델을 예측합니다.\n",
    "        (기존 통합 방식: 모든 소스 후보를 합쳐 1개 예측)\n",
    "\n",
    "        Args:\n",
    "            row: CSV row를 dict로 변환한 값\n",
    "            row_id: 행 식별자(선택)\n",
    "\n",
    "        Returns:\n",
    "            PredictionResult\n",
    "        \"\"\"\n",
    "        queries = build_log_queries_from_row(\n",
    "            row,\n",
    "            soundbar_brand_set=self._brand_set,\n",
    "            include_bt=self.config.include_bt,\n",
    "        )\n",
    "        primary_log = choose_primary_query(queries)\n",
    "        primary = primary_log.query if primary_log else (queries[0].query if queries else \"\")\n",
    "        candidates = self._retrieve_for_queries(queries)\n",
    "        pred: Prediction = self._verifier.verify(queries, candidates)\n",
    "        pred = self._apply_cosine_gate(pred, primary, candidates)\n",
    "        out_primary = primary_log.raw if primary_log else primary\n",
    "        return PredictionResult(\n",
    "            row_id=row_id,\n",
    "            predicted=pred.canonical_model,\n",
    "            confidence=float(pred.confidence),\n",
    "            primary_query=out_primary,\n",
    "            candidates=candidates,\n",
    "            evidence=dict(pred.evidence),\n",
    "        )\n",
    "\n",
    "    def batch_predict(self, df: pd.DataFrame, *, row_id_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        여러 로그 row를 일괄 예측합니다 (기존 통합 방식: 행당 1개 예측).\n",
    "\n",
    "        Args:\n",
    "            df: 로그 DataFrame\n",
    "            row_id_col: row id 컬럼명이 있으면 이를 사용(없으면 인덱스 사용)\n",
    "\n",
    "        Returns:\n",
    "            예측 결과 DataFrame\n",
    "        \"\"\"\n",
    "        rows: list[dict[str, Any]] = df.to_dict(orient=\"records\")\n",
    "        out_rows: list[dict[str, Any]] = []\n",
    "        for i, r in enumerate(rows):\n",
    "            rid = i\n",
    "            if row_id_col and row_id_col in df.columns:\n",
    "                try:\n",
    "                    rid = int(r.get(row_id_col))\n",
    "                except Exception:\n",
    "                    rid = i\n",
    "\n",
    "            res = self.predict_row(r, row_id=rid)\n",
    "            top1_score = res.candidates[0].score if res.candidates else None\n",
    "            top5 = [c.canonical for c in res.candidates[:5]]\n",
    "            top5_candidates = [c.__dict__ for c in res.candidates[:5]]\n",
    "            out_rows.append(\n",
    "                {\n",
    "                    \"row_id\": rid,\n",
    "                    \"predicted_model\": res.predicted if res.predicted is not None else \"UNKNOWN\",\n",
    "                    \"true_model\": \"\",\n",
    "                    \"confidence\": res.confidence,\n",
    "                    \"primary_query\": res.primary_query,\n",
    "                    \"top1_score\": top1_score,\n",
    "                    \"top5\": json_dumps_safe(top5),\n",
    "                    \"top5_candidates\": json_dumps_safe(top5_candidates),\n",
    "                    \"evidence\": json_dumps_safe(res.evidence),\n",
    "                }\n",
    "            )\n",
    "        return pd.DataFrame(out_rows)\n",
    "\n",
    "    def batch_predict_per_source(self, df: pd.DataFrame, *, row_id_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        여러 로그 row를 소스별(BT/HDMI)로 독립 예측합니다.\n",
    "        행당 최대 5개(BT 1 + HDMI 4) 예측 결과를 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            df: 로그 DataFrame\n",
    "            row_id_col: row id 컬럼명이 있으면 이를 사용(없으면 인덱스 사용)\n",
    "\n",
    "        Returns:\n",
    "            예측 결과 DataFrame (row_id, type, predicted_model, true_model, primary_query 등)\n",
    "        \"\"\"\n",
    "        rows: list[dict[str, Any]] = df.to_dict(orient=\"records\")\n",
    "        out_rows: list[dict[str, Any]] = []\n",
    "        out_id = 0\n",
    "        for i, r in enumerate(rows):\n",
    "            rid = i\n",
    "            if row_id_col and row_id_col in df.columns:\n",
    "                try:\n",
    "                    rid = int(r.get(row_id_col))\n",
    "                except Exception:\n",
    "                    rid = i\n",
    "\n",
    "            typeds = build_typed_queries_from_row(\n",
    "                r,\n",
    "                soundbar_brand_set=self._brand_set,\n",
    "                include_bt=self.config.include_bt,\n",
    "            )\n",
    "            for tq in typeds:\n",
    "                res = self.predict_for_query(tq)\n",
    "                top1_score = res.candidates[0].score if res.candidates else None\n",
    "                top5 = [c.canonical for c in res.candidates[:5]]\n",
    "                top5_candidates = [c.__dict__ for c in res.candidates[:5]]\n",
    "                out_rows.append(\n",
    "                    {\n",
    "                        \"id\": out_id,\n",
    "                        \"row_id\": rid,\n",
    "                        \"type\": tq.type_,\n",
    "                        \"predicted_model\": res.predicted if res.predicted is not None else \"UNKNOWN\",\n",
    "                        \"true_model\": \"\",\n",
    "                        \"confidence\": res.confidence,\n",
    "                        \"primary_query\": res.primary_query,\n",
    "                        \"top1_score\": top1_score,\n",
    "                        \"top5\": json_dumps_safe(top5),\n",
    "                        \"top5_candidates\": json_dumps_safe(top5_candidates),\n",
    "                        \"evidence\": json_dumps_safe(res.evidence),\n",
    "                    }\n",
    "                )\n",
    "                out_id += 1\n",
    "        return pd.DataFrame(out_rows)\n",
    "\n",
    "\n",
    "def json_dumps_safe(obj: Any) -> str:\n",
    "    \"\"\"\n",
    "    JSON 직렬화를 안전하게 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        obj: 임의 객체\n",
    "\n",
    "    Returns:\n",
    "        JSON 문자열(실패 시 repr)\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return repr(obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드 및 예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 로드\n",
    "log_res = load_hdmi_bt_log_csv(INPUT_CSV, encoding=\"utf-8\")\n",
    "df_log = log_res.df\n",
    "records = load_soundbar_db_from_py(SOUNDBAR_DB)\n",
    "brand_set = get_brand_set(records)\n",
    "print(\"로그 행 수:\", len(df_log))\n",
    "print(\"사운드바 DB 레코드 수:\", len(records))\n",
    "\n",
    "# Agent 생성 및 일괄 예측\n",
    "config = AgentConfig(\n",
    "    top_n_lexical=200,\n",
    "    top_k=20,\n",
    "    embedding_model_id=\"BAAI/bge-small-en-v1.5\",\n",
    "    use_embeddings=True,\n",
    "    include_bt=True,\n",
    "    accept_score_threshold=0.52,\n",
    "    margin_threshold=0.05,\n",
    "    accuracy_mode=True,\n",
    "    min_cosine_similarity=0.85,\n",
    ")\n",
    "agent = SoundbarModelAgent(SOUNDBAR_DB, config=config)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "pred_df = agent.batch_predict(df_log)\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"예측 완료: {len(pred_df)}행, 소요 시간: {elapsed:.1f}초\")\n",
    "display(pred_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pred_df.to_csv(OUTPUT_PRED, index=False, encoding=\"utf-8\")\n",
    "    print(\"저장 완료:\", OUTPUT_PRED)\n",
    "except OSError as e:\n",
    "    print(\"저장 실패:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}